5. 机器学习基础
    1. 学习算法

        学习的定义：对某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指：通过经验E的改进后，它在任务T上由性能度量P衡量的性能有所提升。

        - 任务T
            样本是指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的特征的集合。

            常见任务：

            - 分类

                计算机需要指定某些输入属于k类中的哪一类。例如对象识别。

            - 输入缺失分类

                在输入的度量无法全部保证时，学习算法需要学习一组不同的函数，分别对应不同的x子集。例如医学疾病监测。

            - 回归

                对给定的输入输出预测值。例如房价预测。

            - 转录

                非结构数据转为文本。例如OCR和语音识别。

            - 机器翻译

                将语言的符号序列转为另一种语言的符号序列。

            - 结构化输出

                将某数据结构映射到输出相关的元素。例如语法分析。

            - 异常检测

                在一组数据中进行筛选，找出不正常或非典型的个体。例如欺诈检测。

            - 合成和采样

                生成一些和训练样本相似的新样本。如画画或作诗。

            - 缺失值填补

                给定一个新样本，补足其中缺失的元素。

            - 去噪

                输入是干净样本的损坏样本，要求预测干净的样本。

            - 密度估计或概率质量函数估计

                通过学习观察到的数据结构对密度估计。

        - 性能度量 P
            
            通过在测试集上进行对准确率和错误率进行评估

        - 经验 E

            机器学习算法分为无监督算法和监督算法。

        - 示例：线性回归

            $$ \hat {y} =w^Tx $$

            向量$x \in \mathbb R^n$作为输入，预测标量$\hat y \in \mathbb R$为输出，$w \in \mathbb R^n$是参数的向量。

            w可以看做是一组决定每个特征如何影响预测的权重。

            $X^{(test)}$表示测试集，回归目标为$y^{(test)}$

            度量模型的性能第一种办法是在测试集上的均方误差。

            $$ MSE_{test}= \dfrac {1}{m} \sum _i(\hat y^{(test)}-y^{(test)})^2_i $$

            这个计算式和求欧几里得距离等价，欧几里得距离如下：
            
            $$ MSE_{test}= \dfrac {1}{m}||\hat y^{(test)}-y^{(test)})||^2_2 $$

            目标是最小化 $MSE_{train}$ ，因此直接求导数为0。

            $$ \nabla_wMSE_{train}=0 $$
            $$ \Rightarrow \nabla_w\dfrac 1m||\hat y^{(train)}-y^{(train)}||^2_2=0 $$
            $$ \Rightarrow \dfrac 1m\nabla_w||\hat X^{(train)}w-y^{(train)}||^2_2=0 $$
            $$ \Rightarrow \nabla_w(X^{(train)}w-y^{(train)})^T(X^{(train)}w-y^{(train)})=0$$
            $$ \Rightarrow \nabla_w(w^TX^{(train)T}X^{(train)}w-2w^TX^{(train)T})y^{(train)}+y^{(train)T}y^{(train)})=0$$
            $$ \Rightarrow 2X^{(train)T}X^{(train)}w-2X^{(train)T}y^{(train)}=0$$
            $$ \Rightarrow w= \dfrac {X^{(train)T}y^{(train)}}{X^{(train)T}X^{(train)}}$$      

            这个方程称为正规方程，实际上的线性回归还要加上额外的参数偏置b，即：

            $$\hat y = w^Tx+b$$      

    2. 容量、过拟合和欠拟合

        在之前未观测到的输入上表现良好的能力被称为泛化。

        在训练集计算的叫训练误差，机器学习优化是指期望泛化误差（测试误差）的优化问题。

        线性回归采用 $\dfrac {1}{m^{train}}||X^{(train)}w-y^{(train)}||_2^2$ 

        但我们实际期望 $\dfrac {1}{m^{test}}||X^{(test)}w-y^{(test)}||_2^2$
            
        按统计学理论，训练集和测试集上，默认为独立同分布假设，就是每个数据集中的样本都是互相独立的，并且训练集和测试集是同分布的。所以可以观察到随机训练误差和测试误差的期望是一致的。

        机器学习的算法效果：

            1. 降低训练误差

            2. 缩小训练误差和测试误差的差距

        针对上面两种目标，引入训练过程中的两个挑战，欠拟合和过拟合。

        可以通过挑战模型的容量，可以控制模型是否偏向于欠拟合或过拟合。模型的容量是指拟合各种函数的能力。

        控制模型训练算法的方法是选择假设空间。广义的线性回归的假设空间包括了多项式函数，而非仅仅线性函数，这样就增加了模型的容量。

        例如：

        一次多项式：

        $$\hat y = wx+b$$

        二次多项式：

        $$\hat y = w_1x^2+w_2x+b$$

        虽然输入的是二次函数，但输出还是线性函数，所以可以解，还可以继续增加，如九次多项式如下：

        $$ \hat y=\sum_{i=1}^9w_ix^i+b $$

        容量高的模型可以解决更复杂的任务，但当容量高于任务所需时，容易发生过拟合。

        学习算法可以从哪些函数族中挑选函数，称为模型的表示容量；在实际过程中，不会去挑一个正好匹配的函数，而是去挑可以大大降低训练误差的函数。并且有时由于算法的不完美，还可能造成学习算法的有效容量可能小于模型族的表示容量。

        提高模型的泛化能力就是奥卡姆剃刀原则，在同样能够解释观察现象的假设中，挑选最简单的那一个。

        统计学习理论提高了量化模型容量的方法叫VC维度。

        对于非参数模型而言，更多的数据会得到更好的泛化性能。

        1. 没有免费的午餐

            机器学习的没有免费的午餐定理表明：在所有可能的数据生成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错误率。也就是说没有一种机器学习算法总比其他的好。

        2. 正则化

            可以加入权重衰减来修改线性回归的训练标准。带权重衰减的线性回归最小化训练集上的均方误差和正则项的和J(w),其偏好于L2范数较小的权重。如下：

            $$J(w)=MSE_{(train)}+\lambda w^Tw$$

            其中$\lambda$是提前设置好的值，越大，越偏好范数越小的权重。最小化J(w)可以看做拟合训练数据和偏好小权重范数之间的权衡，这样会使得解决方案的斜率较小，或者将权重放到较少的特征少，可以提高泛化能力。

            正则化是指修改学习算法，使其降低泛化误差而非训练误差。

        3. 超参数和验证集

            预先设置的参数，用来控制算法的行为，例如正则化 $\lambda$

            通常将难以优化的，或不适合在训练集上学习的都设置为超参数；如果在训练集上训练这些超参数，容易导致过拟合。

            用来挑选超参的数据子集被称为验证集。通常80%用来训练，20%用来验证。

            - 交叉验证

                数据过小会导致测试集的误差太小，区分不出算法的优势。可以使用k-折交叉验证算法。

                将数据分为k份来训练，同时将记录每一个误差，求平均。
            

        4. 估计、偏差和方差

            - 点估计

            用一些独立的点来估计超参用。其中将输入和目标变量之间关系的估计称为函数估计。

            - 偏差

            估计的偏差定义为：

            $$bias(\hat \theta_m)=\mathbb E(\hat \theta_m)-\theta$$

            $\mathbb E(\hat \theta_m)$是期望值，$\theta$表示真实值。

            如果$bias(\hat \theta_m)=0$,那么估计量$\hat \theta_m$被称为无偏，这意味着$\mathbb E(\hat \theta_m)=0$;如果$\lim _{m\rightarrow\infty } bias(\hat \theta_m)=0$, 估计量$\hat \theta_m$被称为渐进无偏。

            例子：

            伯努利分布，考虑 {$x_1,...,x_m$}:

            $$P(x_i;\theta)=\theta^{x_i}(1-\theta)^{(1-x_i)}=\begin{cases}\theta, \ if\ x=1\\ 1-\theta, \ if\ x=0\end{cases}$$

            $\theta$的常用估计量是训练样本的均值：

            $$\hat \theta_m=\dfrac {1}{m}\sum _{i=1}^mX_i$$

            下面来判断这个是否有偏：

            $$
            \begin{aligned}
            bias(\hat \theta_m) &= \mathbb E|\hat \theta_m|-\theta \\
            &=\mathbb E[\dfrac {1}{m}\sum _{i=1}^mX_i]-\theta \\
            &=\dfrac {1}{m}\sum _{i=1}^m\mathbb E[x_i]-\theta\\
            &=\dfrac {1}{m}\sum _{i=1}^m\sum _{x_i=0}^1(x_i\theta^{x_i}(1-\theta)^{(1-x_i)})-\theta \\
            &=\dfrac {1}{m}\sum _{i=1}^m(\theta)-\theta \\
            &=\theta - \theta=0
            \end{aligned} 
            $$
            
            附期望值计算公式：

            $$\mathbb E[X]=\sum _ix_ip_i$$

            所以骰子的期望值为：

            $$\mathbb E[x]=1*\dfrac 16+2*\dfrac 16+3*\dfrac 16+4*\dfrac 16+5*\dfrac 16+6*\dfrac 16=3.5$$

            示例：均值的高斯分布估计