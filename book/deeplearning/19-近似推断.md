深度学习中难以处理的推断问题通常是由于结构化图模型中潜变量的相互作用。但隐藏单元存在连接的半受限玻尔兹曼机或一个深度玻尔兹曼机被分为多层 ，这样就很难处理其后验分布。

1. 把推断视为优化问题

    为了构造这个优化问题，假设需要观察数据的对数概率$\log p(x;\theta)$,如果边缘化消去潜变量h的操作很费时，则难以计算对数概率。这时可以考虑通过计算一个证据下界即$\log p(x;\theta)$的下界$L(x,\theta,q)$，这个也称为负变分自由能。具体定义如下：

    $$L(x,\theta,q)=\log p(x;\theta)-D_{KL}(q(h|x)||p(h|x;\theta))$$

    其中q是关于h的一个任意概率分布。

    $\log p(x;\theta)$和$L(x,\theta,q)$之间的距离为KL散度，KL散度总是为正数，所以 $L$ 总是小于对数概率。

    我们可以选择一个合适的分布q，让 $L$ 好计算，这个分布越和p相近，这个近似越完美。

    因此我们可以将推断问题转为找一个分布q使得 $L$ 最大化问题。在限定q的过程中不求彻底最大化 $L$ ，而是显著的提升 $L$ 。

1. 期望最大化

    一个最大化 $L$ 的方法是期望最大化(EM)，EM算法通过两步交替迭代，直到收敛。

    *   E步，对我们训练样本 $x_i$，令 $q(h_i|x)=p(h_i|x_i;\theta_0)$，这表示q在$\theta_0$下定义，但改变$\theta$时，$p(h|x;\theta)$ 也会相应变化，但$q(h|x)$不变，且等于 $p(h|x;\theta_0)$。

    *   M步，选择优化算法最大化$L(x,\theta,q)$。

    即，第一步更新分布q来最大化$L$,第二步更新$\theta$来最大化$L$。

    这里一个关键特性是当更新到另外一个$\theta$时，还是使用旧的q分布。

1. 最大后验推断和稀疏编码

    推断的定义为给定一些其他变量的情况下，计算某些变量概率分布的过程。
