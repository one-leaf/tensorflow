许多前沿研究都涉及到构建输入的概率模型$p_{model}(x)$。原则上，给定任何其它变量的情况下，这样的模型可以使用概率推断来预测其环境中的任何变量。

许多这样的模型还具有潜变量 h ，其中 $p_{model}(x)=\mathbb E_hP_{model}(x|h)$ 。 这些潜变量提供了表示数据的另一种方式。

本节主要讲基于潜变量的最简单的概率模型：线性因子模型。这些模型用来作为混合模型的组成模块或更大的深度概率模型。

线性因子模型描述如下的数据生成过程：

首先，从一个分布中抽取解释性因子 h：

$$h\sim p(h)$$

其中 $p(h)$ 是一个因子分布，满足 $p(h)=\prod_ip(h_i)$，所以易于采样，接下来，在给定因子的情况下，我们对实值的可观察变量进行采样：

$$x=Wh+b+noise$$

其中噪声$noise$通常是对角化（即在维度上是独立的）且服从高斯分布。

1. 概率PCA和因子分析

    在因子分析中，潜变量的先验是一个方差为单位矩阵的高斯分布：

    $$h\sim \mathcal N(h;0,I)$$

    同时假定在给定$h$的条件下，观察值$x_i$是条件独立的。
    
    这样可以假设噪声是从对角协方差的高斯分布中抽取的，协方差矩阵为：$\psi=diag(\sigma^2)$，其中$\sigma^2=[\sigma_1^2,\sigma_2^2,...,\sigma_n^2]^T$ 表示一个向量，每个元素表示一个变量的方差。

    因此：潜变量的作用是捕获不同观测变量$x_i$之间的依赖关系，可以看出x服从多维正态分布，并满足：

    $$x \sim \mathcal N(x;b,WW^T+\psi)$$

    或等价于：

    $$x=Wh+b+\sigma z$$

    其中 $z \sim \mathcal N(z;0,I)$ 是高斯噪声，之后可以用迭代算法来估计参数 W 和 $\sigma^2$。

    这个概率PCA模型利用了：除了一些微小残余的重构误差（$\sigma^2$）,数据中的大多数变化可以由潜变量$h$描述。当 $\sigma \rightarrow 0$ 时，概率PCA将变为PCA即，h的条件期望等于将x-b投影到W的d列所生成的空间上，同时会导致密度函数在W的空间周围非常尖锐，会导致模型不在一个超平面附近聚集的数据分配非常低的概率。

    [实例代码](./code/13-1.py)

1. 独立成分分析

    独立成分分析(ICA)是一种建模线性因子的方法，旨在将观察到的信号分离成许多潜在信号，这些潜在信号通过缩放和叠加可以恢复成观察数据。这些信号是完全独立的，不仅仅是彼此不相关。

    先人工指定潜在因子$h$的先验$p(h)$，然后模型确定性生成 $x=Wh$，这样可以通过非线性变化来确定 $p(x)$，最后通过最大似然来进行学习。

    ICA的所有变种均要求$p(h)$是非高斯的。这是因为如果$p(h)$是具有高斯风量的独立先验，则W是不可识别的。这里和PCA是不同的。

    ICA多被用作在分离信号。
    
    [实例代码](./code/13-2.py)

1. 慢特征分析

    慢特征分析(SFA)是使用来自时间信号的信息学习不变特征的线性因子模型。基本思想是，与场景中描述作用的单个量度相比，场景的重要特征通常变化得非常缓慢。

    一般向代价函数增加以下项：

    $$\lambda\sum_tL(f(x^{(t+1)}),f(x^{(t)}))$$

    其中，$\lambda$ 是确定慢度正则化的超参数，t是样本时间序列的索引，f是需要正则化的特征提取器，L是测量$f(x^{(t+1)})$和$f(x^{(t)})$之间距离的损失函数，通常用均方误差。

    SFA算法将f(x)定义为线性变换，求解如下优化问题：

    $$\min_\theta\mathbb E_t(f(x^{(t+1)})_i-f(x^{(t)})_i)^2$$

    并且满足：

    $$\mathbb E_tf(x^{(t)})_i=0$$

    和：

    $$\mathbb E_t[f(x^{(t)})^2_i]=1$$

    约束学习特征具有零均值可以避免添加b的偏置，并且具有单位方差的约束可以防止所有的特性都趋向为0的病态解。如果要学习多个特征，还需要要求多个特征之间是无关的。

    在运用SFA之前还可以将x使用非线性的基扩充来学习非线性特征，例如对x做平方，然后得到一个包含$x$和$x^2$两个特征的向量。

    [参考代码](https://github.com/LiangjunFeng/Machine-Learning/blob/master/A10.SFA.py)

1. 稀疏编码

    稀疏编码是一个线性因子模型。通常假设有一个各向同性精度为$\beta$的高斯噪声：

    $$p(x|h)=\mathcal N(x;Wh+b,\frac {1}{\beta}I)$$

    分布$p(h_i)$通常是一个峰值很尖锐且接近0的分布。

    以稀疏惩罚系数$\lambda$为参数的Laplace先验表示为：

    $$p(h_i)=Laplace(h_i;0,\frac 2\lambda)=\frac\lambda4e^{-\frac 12\lambda|h_i|}$$

    相应的Student-t的先验分布表示为：

    $$p(h_i)\propto\frac 1{(1+\frac{h^2_i}{v})^{\frac {v+1}{2}}}$$

    不能用最大似然来训练稀疏编码模型。通常训练过程是在编码数据和训练解码器之间交替进行。

    稀疏编码器不是参数化的编码器，而是一种优化算法。

    即在参数h上施加了L1的范数，这个过程将产生稀疏的结果h。

    $$h=f(x)=argmax(p(h|x))$$

    $$=argmax(log(p(h|x)))$$

    $$=argmin(\lambda||h||_1+\beta||x-Wh||^2_2)$$

    为了训练模型，没有必要保留两个超参数，因此将$\beta$设置为1，交替迭代训练h和W。

    原则上加了稀疏编码方法比不加有更好的泛化能力。

    非参数编码器的主要缺点是因为需要采用迭代算法，所以需要大量的时间来计算。也没有办法通过非参数编码器进行反向传播，所以只能用于监督学习，不能用于无监督学习。

    另外稀疏编码经常产生糟糕的样本，原因是，每个单独的特征可以很好的被学习到，但是隐藏编码值的因子先验会导致模型包括每个生成样本中所有特征的随机子集。这样导致人们开发更深的模型，而在其中最深的编码层施加一个非因子分布。

1. PCA的流形解释

    线性因子模型，包括PCA和因子分析，可以理解为学习一个流形。

    可以将概率PCA理解为高概率的薄饼状区域，即其高斯分布沿着某些轴非常窄，但沿着其它轴是细长的。PCA可以理解为将该薄饼与更高维空间中的线性流形对准。

    从某种程度上说，线性因子模型是最简单的生成模型和学习数据表示的最简单模型。许多模型如线性分类器和线性回归模型可以扩展到深度前馈网络的自编码器网络和深度概率模型，从而具有更强大和更灵活的模型族。

    
