卷积网络也称为卷积神经网络，CNN，是专门用来处理类似网格结构的数据的神经网络。对于时间序列和图像数据都表现优异。

卷积神经网络是指该网络使用了卷积运算，卷积运算是一种特殊的线性运算替代一般的矩阵乘法运算。

1. 卷积运算

    卷积运算通常用*表示，表达的当前的状态与多少时间之前的状态相关性：

    我们称$(x*w)(t)$为$x,w$的卷积，连续定义为

    $$(x*w)(t)=\int_{-\infty}^{\infty}x(a)w(t-a)da$$

    其离散定义为：

    $$(x*w)(t)=\sum_{a=-\infty}^{\infty}x(a)g(t-a)$$

    >例1：制作馒头的速度为x(t)，变质的速度为w(t)，求24小时坏掉的馒头数：
    >
    >一天的生产馒头数为 
    >
    >$$\int_0^{24}x(t)dt$$
    >
    >一天内坏掉的馒头数为馒头的个数乘以腐败的速度，即：
    >
    >$$(x*w)(24)=\int_0^{24}x(t)w(24-t)dt$$

    >例2：两个骰子点数相加为6的概率是多少？
    >
    >定义骰子分别为x,w 则：
    >
    >$$(x*w)(6)=x(1)w(5)+x(2)w(4)+x(3)w(3)+x(4)w(2)+x(5)w(1)$$
    >
    >$$=\sum_{a=1}^5x(a)w(6-a)$$

    在卷积中x称为输入，w叫核函数，输出为特征映射。

    如果在二维上求卷积为：

    $$S(i,j)=(I*K)(i,j)=\sum_m\sum_nI(m,n)K(i-m,j-n)$$

    卷积是可以交换的：

    $$S(i,j)=(K*I)(i,j)=\sum_m\sum_nI(1-m,1-n)K(m,n)$$

    有些神经网络会实现卷积的近似，互相关函数，也俗称为卷积，如下：

    $$S(i,j)=(I*K)(i,j)=\sum_m\sum_nI(i+m,j+n)K(m,n)$$

    二维卷积如下：

    $$\begin{bmatrix} a&b&c&d \\ e&f&g&h \\ i&j&k&l \end{bmatrix}*\begin{bmatrix} w&x \\ y&z \end{bmatrix}$$
    
    $$=\begin{bmatrix} aw+bx+ey+fz&bw+cx+fy+gz&cw+dx+gy+hz \\ ew+fx+iy+jz&fw+gx+jy+zk&gw+hx+ky+lz \end{bmatrix}$$

    离散卷积可以看作矩阵的乘法，然而，这个矩阵的一些元素被限制为必须和另外一些元素相等，称为托普利兹矩阵：

    $$T_{n,n}=\begin{bmatrix} t_0&t_1&\cdots&t_{n-1} \\ t_{-1}&t_0&\cdots&t_{n-2} \\ \vdots&\vdots&\vdots&\vdots \\t_{-n+1}&t_{-n+2}&\cdots&t_0 \end{bmatrix}$$

    对于二维情况，卷积对应一个双重分块循环矩阵，就是不同大小的二维矩阵依照相同大小的卷积核进行两个方向循环的分块乘法得到的新矩阵。

    卷积通常对应一个稀疏的矩阵，核的大小会远小于图像的大小。

1. 动机

    卷积运算有三个重要思想：稀疏交互、参数共享、等变表示。另外卷积提供了一种处理变长输入的办法。

    传统的神经网络都是使用矩阵乘法来建立输入和输出的连接关系，然而卷积使用一个远小于输入大小的核来检测小的有意义的特征。例如，m个输入，n个输出，矩阵乘法需要有m\*n个参数和O(m\*n)个时间复杂度；而卷积如果用k个连接数输出，则只有k\*n个参数和O(k\*n)个时间复杂度。实际使用中k远小于m，这个就是卷积的稀疏交互或稀疏连接或稀疏权重。

    参数共享是在一个模型的多个函数使用相同的参数，传统的神经网络在计算输出时，权重矩阵的每一个元素只使用一次，在卷积网络中核的每一个参数都作用于输入的每一个元素，这样显著的将参数降低到k个，并且k比m小很多个数量级。

    对于卷积，参数共享使的神经网络具有对平移等变的性质，也就是当输入改变时，输出也以同样的方式改变。即将图片先卷积在平移和先平移再卷积的输出效果是一致的。局部平移不变性的重要作用在于对于某个特征关心其是否出现，而不关心其出现的位置。

    有些情况下例如边缘检测，对整个图片进行参数共享是有用的，有点情况例如对已经裁剪的人脸特征进行识别，就可能取不同位置的不同特征，这样就不想对整个图片进行参数共享。

    >例子：
    >
    >对一张280*320的图片进行边缘检测，输出为280*319的边缘二值图片，传统的神经网络需要 280*320*280*319 相当于80亿个元素的矩阵，超过160亿次运算；而卷积只要设置为2个元素的卷积核，进行280*320*3次运算（2次乘法1次加法）就可以完成，存储元素相差40亿倍。

1. 池化

    卷积网络的一个典型层包含三级，第一级卷积层，第二级探测层Relu等激活函数，第三级池化层。

    池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出，有最大、平均、L2范数和基于据中心像素距离的加权平均。不管那种池化，能帮助输入近似不变。

    最大池化只对周围最大值敏感，而不对精确位置敏感，这种称为学习不变性。

    池化综合了全部邻居的反馈，所以可以通过综合池化区域的k个像素的统计特征，而不是单个像素，这样下一层就约减少了k倍的输入。

    池化有助于对处理不同大小的输入，这样可以通过调整池化的偏置大小达到分类层总能接收到相同的统计特征，而不用管最初输入大小。例如池化可以输出为4组综合统计特征，对应4个象限，这样就和图像大小无关了。

    动态池化也是可以的，例如对感兴趣的位置运行聚类算法等。

    池化可能使利用自顶向下信息的网络变得复杂例如玻尔兹曼机和自编码器。

    [演示代码](./code/09-1.py)




    







