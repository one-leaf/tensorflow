卷积网络也称为卷积神经网络，CNN，是专门用来处理类似网格结构的数据的神经网络。对于时间序列和图像数据都表现优异。

卷积神经网络是指该网络使用了卷积运算，卷积运算是一种特殊的线性运算替代一般的矩阵乘法运算。

1. 卷积运算

    卷积运算通常用*表示，表达的当前的状态与多少时间之前的状态相关性：

    我们称$(x*w)(t)$为$x,w$的卷积，连续定义为

    $$(x*w)(t)=\int_{-\infty}^{\infty}x(a)w(t-a)da$$

    其离散定义为：

    $$(x*w)(t)=\sum_{a=-\infty}^{\infty}x(a)g(t-a)$$

    >例1：制作馒头的速度为x(t)，变质的速度为w(t)，求24小时坏掉的馒头数：
    >
    >一天的生产馒头数为 
    >
    >$$\int_0^{24}x(t)dt$$
    >
    >一天内坏掉的馒头数为馒头的个数乘以腐败的速度，即：
    >
    >$$(x*w)(24)=\int_0^{24}x(t)w(24-t)dt$$

    >例2：两个骰子点数相加为6的概率是多少？
    >
    >定义骰子分别为x,w 则：
    >
    >$$(x*w)(6)=x(1)w(5)+x(2)w(4)+x(3)w(3)+x(4)w(2)+x(5)w(1)$$
    >
    >$$=\sum_{a=1}^5x(a)w(6-a)$$

    在卷积中x称为输入，w叫核函数，输出为特征映射。

    卷积本质上是用不同的函数的线性组合，是将(x*w)(t)分为分量为w(t-a)、系数为x(a)的线性组合。傅里叶分析用的是不同的正弦函数的线性组合；

    如果在二维上求卷积为：

    $$S(i,j)=(I*K)(i,j)=\sum_m\sum_nI(m,n)K(i-m,j-n)$$

    卷积是可以交换的：

    $$S(i,j)=(K*I)(i,j)=\sum_m\sum_nI(1-m,1-n)K(m,n)$$

    有些神经网络会实现卷积的近似，互相关函数，也俗称为卷积，如下：

    $$S(i,j)=(I*K)(i,j)=\sum_m\sum_nI(i+m,j+n)K(m,n)$$

    二维卷积如下：

    $$\begin{bmatrix} a&b&c&d \\ e&f&g&h \\ i&j&k&l \end{bmatrix}*\begin{bmatrix} w&x \\ y&z \end{bmatrix}$$
    
    $$=\begin{bmatrix} aw+bx+ey+fz&bw+cx+fy+gz&cw+dx+gy+hz \\ ew+fx+iy+jz&fw+gx+jy+zk&gw+hx+ky+lz \end{bmatrix}$$

    离散卷积可以看作矩阵的乘法，然而，这个矩阵的一些元素被限制为必须和另外一些元素相等，称为托普利兹矩阵：

    $$T_{n,n}=\begin{bmatrix} t_0&t_1&\cdots&t_{n-1} \\ t_{-1}&t_0&\cdots&t_{n-2} \\ \vdots&\vdots&\vdots&\vdots \\t_{-n+1}&t_{-n+2}&\cdots&t_0 \end{bmatrix}$$

    对于二维情况，卷积对应一个双重分块循环矩阵，就是不同大小的二维矩阵依照相同大小的卷积核进行两个方向循环的分块乘法得到的新矩阵。

    卷积通常对应一个稀疏的矩阵，核的大小会远小于图像的大小。

1. 动机

    卷积运算有三个重要思想：稀疏交互、参数共享、等变表示。另外卷积提供了一种处理变长输入的办法。

    传统的神经网络都是使用矩阵乘法来建立输入和输出的连接关系，然而卷积使用一个远小于输入大小的核来检测小的有意义的特征。例如，m个输入，n个输出，矩阵乘法需要有m\*n个参数和O(m\*n)个时间复杂度；而卷积如果用k个连接数输出，则只有k\*n个参数和O(k\*n)个时间复杂度。实际使用中k远小于m，这个就是卷积的稀疏交互或稀疏连接或稀疏权重。

    参数共享是在一个模型的多个函数使用相同的参数，传统的神经网络在计算输出时，权重矩阵的每一个元素只使用一次，在卷积网络中核的每一个参数都作用于输入的每一个元素，这样显著的将参数降低到k个，并且k比m小很多个数量级。

    对于卷积，参数共享使的神经网络具有对平移等变的性质，也就是当输入改变时，输出也以同样的方式改变。即将图片先卷积在平移和先平移再卷积的输出效果是一致的。局部平移不变性的重要作用在于对于某个特征关心其是否出现，而不关心其出现的位置。

    有些情况下例如边缘检测，对整个图片进行参数共享是有用的，有点情况例如对已经裁剪的人脸特征进行识别，就可能取不同位置的不同特征，这样就不想对整个图片进行参数共享。

    >例子：
    >
    >对一张280*320的图片进行边缘检测，输出为280*319的边缘二值图片，传统的神经网络需要 280*320*280*319 相当于80亿个元素的矩阵，超过160亿次运算；而卷积只要设置为2个元素的卷积核，进行280*320*3次运算（2次乘法1次加法）就可以完成，存储元素相差40亿倍。

1. 池化

    卷积网络的一个典型层包含三级，第一级卷积层，第二级探测层Relu等激活函数，第三级池化层。

    池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出，有最大、平均、L2范数和基于据中心像素距离的加权平均。不管那种池化，能帮助输入近似不变。

    最大池化只对周围最大值敏感，而不对精确位置敏感，这种称为学习不变性。

    池化综合了全部邻居的反馈，所以可以通过综合池化区域的k个像素的统计特征，而不是单个像素，这样下一层就约减少了k倍的输入。

    池化有助于对处理不同大小的输入，这样可以通过调整池化的偏置大小达到分类层总能接收到相同的统计特征，而不用管最初输入大小。例如池化可以输出为4组综合统计特征，对应4个象限，这样就和图像大小无关了。

    动态池化也是可以的，例如对感兴趣的位置运行聚类算法等。

    池化可能使利用自顶向下信息的网络变得复杂例如玻尔兹曼机和自编码器。

    [演示代码](./code/09-1.py)

1. 卷积与池化作为一种无限强的先验

    先验的强弱取决于概率密度的集中程序。弱先验具有高的熵值，例如方差很大的高斯分布（方差越大，曲线越平，分布越广，熵越高，越具有不确定性）。弱先验对参数的影响比较少，强先验具有较低的熵值，如方差很小的高斯分布（方差越小，数据越集中，确定性越高，熵越低），这样的分布对参数的影响更大。

    一个无限强的先验就意味，大部分参数的概率都为零，导致这些参数没有被赋值，不管数据对这些参数提供了多大的支持。

    可以将卷积网络看做全连接网络，只是这个网络有一个无限强的先验，即一个隐藏单元的权重必须和它邻居的权重相同，但可以在空间上移动，并且要求范围外的其它权重全部为零。池化也是如此。

    这样的假设可以导出一个结论，卷积和池化可能导致欠拟合。因为任何先验只有合理和正确时才有用，如果有一个模型需要精确依赖空间信息，当在所有特征上使用卷积和池化就会增大训练误差。

    有些网络为了获得平移不变形又防止欠拟合发生时，会设计为一些通道上使用池化，另外一些通道不使用池化。如果有任务涉及到相隔很远的信息进行合并时，卷积的先验也是不正确的。

    另外一个结论就是，如果需要对比模型的效果，只能在卷积网络中进行比较，因为不使用卷积的模型当数据完全不同时，还有可能会继续学习。

1. 基本卷积函数的变体    

    神经网络中的卷积，通常由多个并行卷积组成，因为单个核的卷积只能提取一种类型的特征，尽管作用在多个空间位置上。我们通常希望网络的每一层能够在多个位置提取多种类型的特征。

    图片都是4维的张量，【颜色通道，高，宽，批处理索引】，颜色通道通常为【1，3，4】对应【灰度，RGB，CMYK】（tensoflow 通常为：[batch_size, height, width, channels]）

    假定有个4维的核张量K，每个元素为$K_{i,j,k,l}$，i对应输出，j对应输入，k和l对应k行l列的偏置。假定V为观测数据，每个元素为$V_{i,j,k}$，i为色彩通道，j为高度，k为宽度，则Z为对K*V的卷积如下：

    $$Z_{i,j,k}=\sum_{l,m.n}[V_{l,j+m-1,k+n-1}K_{i,l,m,n}]$$

    如果想跳过一些位置，降低计算开销，当然特征提取会差一点。这个称为对卷积输出的下采样，即先格s个像素进行采样，如下：

    $$Z_{i,j,k}=\sum_{l,m.n}[V_{l,(j-1)*s+m,(k-1)*s+n}K_{i,l,m,n}]$$

    另外为了保证输出和输入同样大小，可以将输入的V用零填充，使其加宽。如果不加宽，每次卷积会减少（m-(k-1)）个像素，称为valid有效卷积；如果加宽，称为same相同卷积。但即使相同卷积，边缘的部分采用还是不足，因此将边缘继续填充到和核一致，称为full全卷积，输出图片为(m+k-1)。这里m为图片的尺寸，k为核尺寸。但全卷积受到填充零的影响，像素更少，导致学道更困难，通常填充零的数量处于same卷积和full卷积之间的某个位置。

    有时希望卷积不共享参数，则参与6维的W张量，即【输出通道i，输出行j，输出列k，输入的通道l，输入的行偏置m，输入的列偏置n】，称为非共享卷积：

    $$Z_{i,j,k}=\sum_{l,m.n}[V_{l,j+m-1,k+n-1}W_{i,j,k,l,m,n}]$$

    平铺卷积对卷积层和局部连接层做了折中，引入了一组t个不同的核，这样可以循环利用。

    $$Z_{i,j,k}=\sum_{l,m.n}[V_{l,j+m-1,k+n-1}W_{i,l,m,n,j\%t+1,k\%t+1}]$$

    卷积的反向传播计算：

    定义步长为s，卷积核为K，图像V，则卷积网络为c(K,V,s)，我们想最小化J(V,K)。前向传播时c输出到Z，Z输出到分类网络用于计算损失函数J，反向传播会得到一个张量G，$G_{i,j,k}=\frac {\partial}{\partial Z_{i,j,k}}J(V,K)$

    对G求导得：

    $$g(G,V,s)_{i,j,k,l}=\frac {\partial}{\partial K_{i,j,k,l}}J(V,K)=\sum_{m,n}G_{i,m,n}V_{j,(m-1)*s+k,(n-1)*s+l}$$

    如果这一层不是底层，需要对V求梯度，进一步反向传播：

    $$h(K,G,s)_{i,j,k}=\frac {\partial}{\partial V_{i,j,k}}J(V,K)=\sum_{l,m}\sum_{n,p}\sum_qK_{q,i,m,p}G_{q,l,n}$$

    通常的卷积运算在激活函数前会加入偏置，偏置也是共享参数，可以允许模型来校正图像中不同位置的统计差异，例如边缘的输入较少，所以需要较大的偏置。

1. 结构化输出

    卷积网络可以输出高维结构化对象，而不仅仅是回归或分类。例如模型可以产生张量S，其中$S_{i,j,k}$是输入的(j,k)像素属于类i的概率。（图片边缘检测，用于分割）

    解决输出图片比输入小的问题，一避免用池化或使用1的池化，二直接就产生一种低分辨率的标签来用于训练。

    对图片像素进行标记还有一种策略是先产生像素的原始猜测，然后使用相邻像素的交互来修正原始猜测，使用相同的卷积来重复这个修正步骤，这样形成了一种特殊的循环神经网络结构。

    完成预测后，可以用来做图片在区域上的分割。

1. 数据类型

    卷积使用的数据通常都是多通道，每个通道是时间或空间上的观测量。

    卷积网络可以处理具有可变的空间尺度的输入，这样针对不同大小的图片，直接就可以使用卷积，如果是线性网络，则需要考虑图片的大小。神州卷积的输出也可以是随着输入大小的不同提供对应大小不同的输出。如果必须产生一些固定大小的输出，可以通过插入池化层，而池化层的大小和输入的大小成比例，就可以保持固定数量的池化输出。

1. 高效的卷积算法

    可以选择适当的卷积算法来加速卷积。卷积等效于傅里叶变换，将输入和核都转到频域，执行逐点相乘，再用傅里叶变换回时域。这种办法某些领域会更高效。

    当一个核等于d个向量的外积时，该核被称为可分离的。等价于组合d个1维卷积，这样组合的方法会远快于正常的卷积。组合方法的时间为$O(w*d)$，而多维卷积需要$O(w^d)$的运行时间。

1. 随机或无监督的特征

    卷积网络训练中最昂贵的部分就是学习特征。减少卷积网络训练成本的一种方式就是使用那些不是由监督方式训练得到的特征。

    有三种策略不通过监督学习得到卷积核：一、随机初始化；二、手工设计；三、使用无监督的标准来学习核，例如先用k聚类算法将图像分块，然后用每个学到的中心点作为卷积核。然后直接使用这个网络提取的特征作为训练集来训练分类网络就基本都是属于凸优化问题。

    随机初始化权重的表现也不错，会自然变得有频率选择性和平移不变性。这个是一个种廉价的办法，首先只训练最后一层来评估网络结构的性能，最后选择最好的网络结构来训练整个网络。

    有一种中间方法来学习特征，不用每次都完整计算整个梯度，和多层感知机一样，先单独训练第一层，然后一次性提取第一层的特征来训练第二层，依次类推。这样可以训练非常大的模型，之前在计算力有限的情况下很流行。

1. 卷积网络的神经科学基础














    




    







