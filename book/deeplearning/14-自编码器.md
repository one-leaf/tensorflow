自编码器是神经网络的一种，能将输入复制到输出。我们不能将自编码器设计成输入和输出完全相等，我们向自编码器中加入一些约束，使其只能近似的复制，并只能复制与训练数据相似的输入。这些约束强制模型考虑输入数据的哪些部分需要被优先复制，从而学习到数据的有用特性。

传统的自编码器用于降维或特征学习，近来，自编码器与潜变量模型理论的联系将自编码器带到了生成式建模的前沿。

自编码器可以被看做是前馈网络的一个特例，可以使用小批量梯度下降法进行训练。与前馈网络不同的是，自编码器还可以用再循环训练，不过这种算法很少用于机器学习应用。

1. 欠完备自编码器

    从自编码器获得有用特性的一种方法是h的维度比x小，这种编码维度小于输入维度的自编码器称为欠完备自编码器。学习欠完备自编码器将获得训练数据中最显著的特征。

    损失函数为

    $$L(x,g(f(x)))$$

    其中L为损失函数，常用均方误差。

    当解码器是线性的，且L是均方误差，最终欠完备的自编码将学习出和PCA相同的生成子空间。
    
    因此引入非线性的f和g后，理论上会学习到更强大的PCA非线性空间，但实际如果编码器和解码器被赋予了过大的容量后，自编码器只会复制，并不能捕捉到任何数据分布的有用信息。因此自编码器的容量不能过大，否则将无法学习到数据集的任何有用信息。

1. 正则自编码器

    正则自编码器使用的损失函数可以鼓励模型学习到其他的特性，而不必限制使用浅层的编码器和解码器，或小的编码维度来限制模型的容量。这些特性包括稀疏表示、表示的小导数以及对噪声或输入缺失的鲁棒性。

    1. 稀疏自编码器

        稀疏自编码器直接在训练时结合编码层的稀疏惩罚和重构误差：

        $$L(x,g(f(x)))+\Omega(h)$$ 

        其中g(h)是解码器的输出，h是编码器的输出，即h=f(x)。

        稀疏自编码器一般用来学习特征，以便用于像分类这样的任务。

        可以认为整个稀疏自编码框架是对带有潜变量的生成模型的近似最大似然训练，而不将稀疏惩罚视为复制任务的正则化。

        定义带有可变变量x和潜变量h的模型，$p(x,h)=p(h)p(x|h)$。我们将$p(h)$视为模型关于潜变量的先验分布，即模型看到x的信念先验。

        对数似然函数可分解为：

        $$logp(x)=log\sum_hp(h,x)$$

        可以理解为自编码器使用一个高似然值h的点估计近似这个总和。h是参数编码器的输出，而不是从优化结构推断出最可能的h。因此得最大化如下：

        $$logp(h,x)=logp(h)+logp(x|h)$$

        其中$logp(h)$项能被稀疏诱导，如选择Laplace先验：

        $$p(h)=\frac \lambda2 e^{-\lambda|h_i|}$$

        对应的是绝对值稀疏惩罚，将对数先验表示为绝对值惩罚，得到：

        $$\Omega(h)=\lambda\sum_i|h_i|$$

        $$-logp(h)=\sum_i(\lambda|h_i|-log\frac \lambda2)=\Omega(h)+const$$

        $const$是关于$\lambda$的常量，可以丢弃不影响学习。从最终似然学习的结果来看，稀疏惩罚完全不是一个正则项，仅仅影响到潜变量的分布。所以可以得出稀疏惩罚导致的潜变量可以解释输入。

    1. 去噪自编码器

        传统的自编码器最小化以下目标：

        $$L(x,g(f(x)))$$

        L是损失函数，惩罚g(f(x))与x的差异，例如L为L2范数。如果模型容量过大，会导致x与g恒等。

        去噪自编码的最小目标为：

        $$L(x,g(f(\tilde x)))$$

        其中 $\tilde x$ 是带噪声的x副本，这样去噪自编码器需要撤销这些噪声，而不是简单的复制。

    1. 惩罚导数作为正则

        这种和稀疏自编码器相似：

        $$L(x,g(f(x)))+\Omega(h,x)$$

        但$\Omega$和稀疏自编码器中的不同，为：

        $$\Omega(h,x)=\lambda\sum_i||\nabla_xh_i||^2$$

        这种迫使模型在学习一个x的变化小的目标时，也没有太大的变化。

        由于这个惩罚只对训练数据适用，所以它可以反映训练数据分布信息的特征。

        这样正则化的自编码被称为收缩自编码器，

1. 表示能力、层的大小和深度

    自编码器通常只有单层的编码器和解码器，不过深度编码器能提供更多的优势。

    深度的网络可以添加约束，比如增加稀疏自编码，但浅层不行。如果给定足够多的隐藏单元，可以以任意精度去近似任意输入到输出的映射。

    另外，深度自编码器比浅层或线性自编码器产生更好的压缩效率。

    一般训练深度自编码的方法是，用浅层的自编码器来预训练相应的深度架构。

1. 随机编码器和解码器

    自编码器本质上是一个前馈网络，可以用和前馈网络一样的损失函数和输出单元。

    可以引入负对数似然来训练。给定一个隐藏编码h，可以认为解码器提供了一个条件分布$p(x|h)$，然后根据最小化$-log p(x|h)$来训练编码器。如果x是实值，则负对数似然为均方误差；如果x为二值，则为sigmoid伯努利分布；如果x为离散的，则对应softmax分布。

    这样进一步将编码函数f(x)推广为编码分布p(h|x)，则：

    我们将任何 p(h,x)定义一个随机编码器：

    $$p_{encoder}(h|x)=p_{model}(h|x)$$

    再定义一个随机解码器：

    $$p_{decoder}(x|h)=p_{model}(x|h)$$

    通常情况，编码器的分布和解码器的分布没有必要和联合分布$p_{model}(x|h)$相容。如果保证足够的容量和样本，也能使得用于去噪声自编码器的编码器和解码器渐进相容。

1. 去噪自编码器详解

    去噪自编码器(DAE)是一类接受损坏数据作为输入，并训练来预测原始未被损坏数据作为输出的自编码器。

    - 从训练数据中采一个训练样本 x

    - 从 $C(\tilde x|x=x)$ 采一个损坏样本 $\tilde x$

    - 将 $(x,\tilde x)$ 作为训练样本来估计自编码器的重构分布 $p_{reconstruct}(x|\tilde x)=p_{decoder}(x|h)$，其中h是编码器$f(\tilde x)$的输出，$p_{decoder}$根据解码函数$g(h)$定义。

    - 然后简单的用负对数似然$-\log p_{decoder}(x|h)$进行基于梯度法的近似最小化。

    1. 得分估计

        得分匹配是最大似然的替代，它提供了概率分布的一致估计，促使模型在各个数据点x上获得与数据分布相同的得分。得分是一个如下梯度场：

        $$\nabla_x\log p(x)$$

        













