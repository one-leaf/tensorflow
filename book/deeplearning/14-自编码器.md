自编码器是神经网络的一种，能将输入复制到输出。我们不能将自编码器设计成输入和输出完全相等，我们向自编码器中加入一些约束，使其只能近似的复制，并只能复制与训练数据相似的输入。这些约束强制模型考虑输入数据的哪些部分需要被优先复制，从而学习到数据的有用特性。

传统的自编码器用于降维或特征学习，近来，自编码器与潜变量模型理论的联系将自编码器带到了生成式建模的前沿。

自编码器可以被看做是前馈网络的一个特例，可以使用小批量梯度下降法进行训练。与前馈网络不同的是，自编码器还可以用再循环训练，不过这种算法很少用于机器学习应用。

1. 欠完备自编码器

    从自编码器获得有用特性的一种方法是h的维度比x小，这种编码维度小于输入维度的自编码器称为欠完备自编码器。学习欠完备自编码器将获得训练数据中最显著的特征。

    损失函数为

    $$L(x,g(f(x)))$$

    其中L为损失函数，常用均方误差。

    当解码器是线性的，且L是均方误差，最终欠完备的自编码将学习出和PCA相同的生成子空间。
    
    因此引入非线性的f和g后，理论上会学习到更强大的PCA非线性空间，但实际如果编码器和解码器被赋予了过大的容量后，自编码器只会复制，并不能捕捉到任何数据分布的有用信息。因此自编码器的容量不能过大，否则将无法学习到数据集的任何有用信息。

1. 正则自编码器

    正则自编码器使用的损失函数可以鼓励模型学习到其他的特性，而不必限制使用浅层的编码器和解码器，或小的编码维度来限制模型的容量。这些特性包括稀疏表示、表示的小导数以及对噪声或输入缺失的鲁棒性。

    1. 稀疏自编码器

        稀疏自编码器直接在训练时结合编码层的稀疏惩罚和重构误差：

        $$L(x,g(f(x)))+\Omega(h)$$ 

        其中g(h)是解码器的输出，h是编码器的输出，即h=f(x)。

        稀疏自编码器一般用来学习特征，以便用于像分类这样的任务。

        可以认为整个稀疏自编码框架是对带有潜变量的生成模型的近似最大似然训练，而不将稀疏惩罚视为复制任务的正则化。

        定义带有可变变量x和潜变量h的模型，$p(x,h)=p(h)p(x|h)$。我们将$p(h)$视为模型关于潜变量的先验分布，即模型看到x的信念先验。

        对数似然函数可分解为：

        $$logp(x)=log\sum_hp(h,x)$$

        可以理解为自编码器使用一个高似然值h的点估计近似这个总和。h是参数编码器的输出，而不是从优化结构推断出最可能的h。因此得最大化如下：

        $$logp(h,x)=logp(h)+logp(x|h)$$

        其中$logp(h)$项能被稀疏诱导，如选择Laplace先验：

        $$p(h)=\frac \lambda2 e^{-\lambda|h_i|}$$

        对应的是绝对值稀疏惩罚，将对数先验表示为绝对值惩罚，得到：

        $$\Omega(h)=\lambda\sum_i|h_i|$$

        $$-logp(h)=\sum_i(\lambda|h_i|-log\frac \lambda2)=\Omega(h)+const$$

        $const$是关于$\lambda$的常量，可以丢弃不影响学习。从最终似然学习的结果来看，稀疏惩罚完全不是一个正则项，仅仅影响到潜变量的分布。所以可以得出稀疏惩罚导致的潜变量可以解释输入。

    1. 去噪自编码器

        传统的自编码器最小化以下目标：

        $$L(x,g(f(x)))$$

        L是损失函数，惩罚g(f(x))与x的差异，例如L为L2范数。如果模型容量过大，会导致x与g恒等。

        去噪自编码的最小目标为：

        $$L(x,g(f(\tilde x)))$$

        其中 $\tilde x$ 是带噪声的x副本，这样去噪自编码器需要撤销这些噪声，而不是简单的复制。

    1. 惩罚导数作为正则

        这种和稀疏自编码器相似：

        $$L(x,g(f(x)))+\Omega(h,x)$$

        但$\Omega$和稀疏自编码器中的不同，为：

        $$\Omega(h,x)=\lambda\sum_i||\nabla_xh_i||^2$$

        这种迫使模型在学习一个x的变化小的目标时，也没有太大的变化。

        由于这个惩罚只对训练数据适用，所以它可以反映训练数据分布信息的特征。

        这样正则化的自编码被称为收缩自编码器，

1. 表示能力、层的大小和深度

    自编码器通常只有单层的编码器和解码器，不过深度编码器能提供更多的优势。

    深度的网络可以添加约束，比如增加稀疏自编码，但浅层不行。如果给定足够多的隐藏单元，可以以任意精度去近似任意输入到输出的映射。

    另外，深度自编码器比浅层或线性自编码器产生更好的压缩效率。

    一般训练深度自编码的方法是，用浅层的自编码器来预训练相应的深度架构。

1. 随机编码器和解码器

    自编码器本质上是一个前馈网络，可以用和前馈网络一样的损失函数和输出单元。

    可以引入负对数似然来训练。给定一个隐藏编码h，可以认为解码器提供了一个条件分布$p(x|h)$，然后根据最小化$-log p(x|h)$来训练编码器。如果x是实值，则负对数似然为均方误差；如果x为二值，则为sigmoid伯努利分布；如果x为离散的，则对应softmax分布。

    这样进一步将编码函数f(x)推广为编码分布p(h|x)，则：

    我们将任何 p(h,x)定义一个随机编码器：

    $$p_{encoder}(h|x)=p_{model}(h|x)$$

    再定义一个随机解码器：

    $$p_{decoder}(x|h)=p_{model}(x|h)$$

    通常情况，编码器的分布和解码器的分布没有必要和联合分布$p_{model}(x|h)$相容。如果保证足够的容量和样本，也能使得用于去噪声自编码器的编码器和解码器渐进相容。

1. 去噪自编码器详解

    去噪自编码器(DAE)是一类接受损坏数据作为输入，并训练来预测原始未被损坏数据作为输出的自编码器。

    - 从训练数据中采一个训练样本 x

    - 从 $C(\tilde x|x=x)$ 采一个损坏样本 $\tilde x$

    - 将 $(x,\tilde x)$ 作为训练样本来估计自编码器的重构分布 $p_{reconstruct}(x|\tilde x)=p_{decoder}(x|h)$，其中h是编码器$f(\tilde x)$的输出，$p_{decoder}$根据解码函数$g(h)$定义。

    - 然后简单的用负对数似然$-\log p_{decoder}(x|h)$进行基于梯度法的近似最小化。

    1. 得分估计

        得分匹配是最大似然的替代，它提供了概率分布的一致估计，促使模型在各个数据点x上获得与数据分布相同的得分。得分是一个如下梯度场：

        $$\nabla_x\log p(x)$$

        就是用重构模型$g(f(x))-\tilde x$得到向量场，然后该向量场将上面的得分估计为一个乘性因子，即重构误差均方根的平均。就是通过向量场等于某个函数的梯度来计算得分。

        对这类采用高斯噪声和均方误差作为重构误差的特定去噪自编码器的去噪训练过程，与RBM的无向概率模型是等价的。

    1. 历史展望

        采用MLP去噪的很大用处是学习到一个好的内部表示，再用于预训练更深的网络。

        与稀疏自编码器、稀疏编码、收缩自编码器等正则化编码器类似，DAE的动机也是允许学习容量很高的编码器，同时防止学习到一个无用的恒等函数。

1. 使用自编码器学习流形

    流形的一个重要特征是切平面的集合。d维流形上的一点x，切平面由能张成流形上允许变动的局部反向的d维基向量给出。

    训练过程有以下两种推动力的折中。

    1. 学习训练样本x的表示h使得x能通过解码器近似地从h中恢复。

    1. 满足约束或正则惩罚

    结果是导致，自编码器必须有能力表示重构训练实例所需的变化。这样如果该数据生成分布靠近一个低维流形，自编码器能隐式产生捕捉这个流形局部坐标系的表示，仅在x周围关于流形的相切变化需要对应于h=f(x)中的变化。这样导致编码可以学习到映射只对沿着流形方向的变化敏感，并对其他方向不敏感。

    流形学习大多专注于试图捕捉到这些流形的无监督学习过程。

1. 收缩自编码器

    收缩自编码器是在编码h=f(x)的基础上添加了显示的正则项，鼓励f的导数尽可能小。

    $$\Omega(h)=\lambda \left\| \frac{\partial f(x)}{\partial x} \right\| ^2_F$$

    惩罚项 $\Omega$ 是编码器函数相关偏导数雅可比矩阵的平方F范数。

    去噪自编码器和收缩自编码器在小高斯噪声的限制下，惩罚项是等价的。也就是说，这两种自编码器都支持抵抗极小的输入扰动。

    训练一个困难度是，在很深的自编码器的情况下将难以计算。解决方法是分别训练一系列单层的自编码器，并且每个被训练为重构前一个自编码器的隐藏层。因为每个层是局部收缩的，所以最终整体也是收缩的。虽然和整体训练同样的正则项的结果不同，但依然抓住了理想的定性特征。

    另外一个困难度是，如果不对解码器强加一些约束，收缩惩罚可能导致无用的结果。例如如果$\lambda$同时乘以输入和输出项，但极小值时，会导致惩罚项为0，最终学习不到任何分布信息。解决方案是，将g的权重设置为f权重的转置。

1. 预测稀疏分解

    预测稀疏分解（PSD）是稀疏编码和参数化自编码器的混合模型。参数化编码器被训练为能预测迭代推断的输出。

    损失函数为：

    $$\left\|x-g(h) \right\|^2+\lambda|h|_1+\gamma \left\|h-f(x) \right\|^2$$

    预测稀疏分解是学习近似推断的一个例子。

    这个模型的好处是学习容易，可以堆叠，和用于初始化其他训练准则的深度网络。

1. 自编码器的应用

    自编码器器已成功用于降维和信息检索任务。

    低维表示可以提高许多任务的性能。包括分类，信息检索等。

    信息检索可以训练生成一个低维且二值的编码，这样可以将相同二值编码的数据库条目作为查询条件进行信息检索。这个称为语义哈希，常用于文本和图像。




















