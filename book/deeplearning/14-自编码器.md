自编码器是神经网络的一种，能将输入复制到输出。我们不能将自编码器设计成输入和输出完全相等，我们向自编码器中加入一些约束，使其只能近似的复制，并只能复制与训练数据相似的输入。这些约束强制模型考虑输入数据的哪些部分需要被优先复制，从而学习到数据的有用特性。

传统的自编码器用于降维或特征学习，近来，自编码器与潜变量模型理论的联系将自编码器带到了生成式建模的前沿。

自编码器可以被看做是前馈网络的一个特例，可以使用小批量梯度下降法进行训练。与前馈网络不同的是，自编码器还可以用再循环训练，不过这种算法很少用于机器学习应用。

1. 欠完备自编码器

    从自编码器获得有用特性的一种方法是h的维度比x小，这种编码维度小于输入维度的自编码器称为欠完备自编码器。学习欠完备自编码器将获得训练数据中最显著的特征。

    损失函数为

    $$L(x,g(f(x)))$$

    其中L为损失函数，常用均方误差。

    当解码器是线性的，且L是均方误差，最终欠完备的自编码将学习出和PCA相同的生成子空间。
    
    因此引入非线性的f和g后，理论上会学习到更强大的PCA非线性空间，但实际如果编码器和解码器被赋予了过大的容量后，自编码器只会复制，并不能捕捉到任何数据分布的有用信息。因此自编码器的容量不能过大，否则将无法学习到数据集的任何有用信息。

1. 正则自编码器

    正则自编码器使用的损失函数可以鼓励模型学习到其他的特性，而不必限制使用浅层的编码器和解码器，或小的编码维度来限制模型的容量。这些特性包括稀疏表示、表示的小导数以及对噪声或输入缺失的鲁棒性。

    1. 稀疏自编码器

        稀疏自编码器直接在训练时结合编码层的稀疏惩罚和重构误差：

        $$L(x,g(f(x)))+\Omega(h)$$ 

        其中g(h)是解码器的输出，h是编码器的输出，即h=f(x)。

        稀疏自编码器一般用来学习特征，以便用于像分类这样的任务。

        可以认为整个稀疏自编码框架是对带有潜变量的生成模型的近似最大似然训练，而不将稀疏惩罚视为复制任务的正则化。

        定义带有可变变量x和潜变量h的模型，$p(x,h)=p(h)p(x|h)$。我们将$p(h)$视为模型关于潜变量的先验分布，即模型看到x的信念先验。

        对数似然函数可分解为：

        $$logp(x)=log\sum_hp(h,x)$$

        可以理解为自编码器使用一个高似然值h的点估计近似这个总和。h是参数编码器的输出，而不是从优化结构推断出最可能的h。因此得最大化如下：

        $$logp(h,x)=logp(h)+logp(x|h)$$


