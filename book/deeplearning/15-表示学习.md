可以将监督学习训练的前馈网络视为表示学习的一种形式。

前馈网络没有对学成的中间特性明确强加条件，其它的表示学习往往会以某种特定的方式明确设计表示。如更多的独立性分布，将表示向量h中元素之间的相互独立的目标函数等。

表示学习提供了无监督学习和半监督学习的一种方法。

1. 贪心逐层无监督预训练

    这个过程是训练一个无监督学习获得输入的分布表示，用于另外的具有相同输入的监督学习任务。

    贪心逐层无监督预训练依赖单层表示学习算法，每一层使用无监督学习预训练，将前一层的输出作为输入，输出数据的新的表示。这个新的表示是更简单的。

    这种贪心逐层训练过程，用于规避监督学习中神经网络难以训练多层的问题。

    贪心逐层训练每次处理一层网络，训练当前层时，保持前面的网络层不变。低层最先训练，并且不会再引入高层网络后再进行调整。当全部完成后，再到监督学习算法中对整个网络进行监督精调。

    1. 何时以及为何无监督预训练有效

        无监督预训练结合了两种不同的想法。第一，利用深度神经网络对初始参数的选择，对模型起到正则化效果。第二，学习输入的分布有助于学习从输入到输出的映射。

        无监督预训练对词向量很有用，因为初始的词向量之间都是等距的。

        无监督预训练对样本标注很少的情况下很有用。

        无监督预训练对学习函数非常复杂的情况下很有用。

        原因是，一，无监督预训练将参数引入到其他方法可能探索不到的领域；二，无监督学习将参数初始化到它们不易逃逸的区域。

        无监督预训练的缺点，一，没有一种明确的方法来调整无监督预训练阶段正则化的强度，其超参数，效果只能事后独立，无法提前预测。二，具有两个独立的训练阶段导致每个训练阶段具有各自的超参数，第二阶段无法在第一阶段中期进行，所以验证需要花费更长的时间。

        如今除了词向量，其他大部分模型都不再进行无监督预训练。因为基于Dropout和批标准化来进行的正则化，可以在很多任务上都超过人类级别的性能。

1. 迁移学习和领域自适应

    迁移学习和领域自适应是利用一个场景中学习到的内容去改善另外一个场景中泛化情况。

    另外不同任务之间共享的有可能不是输入的语义，有可能是输出的语义。例如语音识别系统，网络的输出层是需要识别的音素输出到输出层产生句子，但在输入附近是针对不同的方言产生不同的音素版本。即，如果底层结构是面向任务的，则上层结构是共享的，底层结构学习将面向特定任务的输入转化为通用特征。

    在领域自适应的相关情况下，在每个情景之间任务都是相同，但输入的分布稍有不同。例如针对电影评价的情感分析，转到针对小说的情感分析，由于词汇和风格会因为领域的不同而有差别，这导致跨域的泛化训练很困难。而针对这个情况使用简单的无监督去噪自编码器可以很好的解决。

    还有一个相关的问题是概念漂移，即数据的分布随时间的变化而变化，可以看做一种迁移学习。通常，我们将概念漂移和迁移学习都视为多任务学习的特定形式。

    迁移学习的两种极端形式是一次学习和零次学习，或称为零数据学习。例如如果文本已经很好的描述了猫的形状，即使在没有见过猫的情况下，也能识别图像中的动物是猫。零次学习要求T被表示为某种形式的泛化。

    例如翻译，学习了语言A的分布和语音B的词向量分布，同时学习了语言A翻译到语言B的句子，这样就可以形成单词的翻译。

    多模态学习也是如此，通过学习三组参数(x的分布表示，y的分布表示，以及两个表示之间的关系)，这样一个表示中的概念被锚定在另一个表示中，反之亦然。

1. 半监督解释因果关系

    表示学习的一个重要问题是：什么原因能够使一个表示比另外一个表示更好？理想表示中的特征对应到观测数据的潜在成因，特征空间中不同的特征对应不同的原因，从而表示能够区分这些原因。

    在表示学习中，通常关注易于建模的表示，但例如稀疏网络虽然可以分离出潜在因素的表示，但不宜建模。

    那么，为什么要用无监督表示学习来替换半监督学习的一个原因是，如果我们能够获得观察结果基本成因的解释，就很容易分离个体属性。因为无监督表示学习都是暴力求解，这样分解潜变量比较容易。当然现实中，暴力求解比较困难，通常都是同时用无监督学习和监督学习信号，用来捕获最相关的变化因素，当然也可以直接用无监督学习更大的规模的表示。

    无监督学习的另外一个原因是选择一个更好的确定哪些潜在因素最为关键的定义。之前的自编码模型通常采用均方误差进行学习，这样导致小对象的作用被削弱。所以为了解决这个问题，引入了生成式对抗网络（GAN）。对抗网络可以生成什么是显著的潜在因素。

    学习潜在因素的好处是，因果机制是不变的，所以通过学习试图恢复成因向量h和p(x|h)的生成模型，期望最后的模型对所有种类的变化有更好的泛化和鲁棒性。

1. 分布式表示

    由很多元素组合的表示，这些元素之间可以设置成可分离的是表示学习的最重要的工具之一。

    强大的原因是可以用k个值n个特征去描述$k^n$个不同的概念。

    具有多个隐藏单元的神经网络和具有多个潜变量的概率模型都是利用了分布式表示的策略。

    非分布式表示的类型有：聚类算法、K-最近邻算法、决策树、高斯混合体和专家混合体、具有高斯核的核机器、基于n-gram的语言模型。

    非分布式的优点是，给定足够的参数，它能够拟合一个训练集，而不需要复杂的优化算法。缺点是模型只能通过平滑先验来局部的泛化，因此学习波峰波谷多余样本的复杂函数时，这种方法就不可行了。

    分布式表示有个重要的概念是，由不同概念之间的共享属性而产生的泛化。所以用嵌入word的单词比one-hot的单词模型在神经语言模型上泛化表现较好，每个单词的距离是不同的，而非one-hot，one-hot的距离是相同的。

    分布式表示表现更佳的原因是：1、非分布式仅仅在平滑假设的情况下才能泛化，而分布式表示除了平滑之外，还具有其他的一些特征规律（例如CNN最大池化可以不考虑目标在图片中的位置）。2、可以对分布式表示中的每一个特征使用非线性的特征提取器，而不是线性单元，这样可以更关注特征空间。3、大部分的分类都是线性可分的，而不需要XOR非线性逻辑类别，这样实际上限制了任意函数的映射，这种情况下分布式表示会表现更好，在学习其中的每个特征时，不需要知道其它特征的配置。

1. 得益于深度的指数增益

    在许多不同的情景中已经证明，非线性和重用特征层次结构的组合来计算，可以使分布式表示获得指数级加速和获得统计效率的指数级提升。

    如果深度不够，就可能需要数量庞大的隐藏单元。足够深的前馈网络会比深度不够的网络具有指数级的优势。可以通过对最小深度的要求来避免模型规模太大。

1. 提供发现潜在原因的线索

    一个理想的表示能够区分生成数据变化的潜在因果因子。为了有助于学习潜在原因的线索，引入先验信息，例如正则化策略，其通用正则化策略列表如下：

    - 平滑 

        对于单位$d$和小量$\epsilon$,有$f(x+\epsilon d)\approx f(x)$,这个假设可以允许机器学习从样本泛化到输入空间附件的点，不过这个假设不能克服维度灾难的问题。

    - 线性

        假定一些变量之间的关系是线性的，这样使得算法可以预测远离观测数据的点，但有时会导致一些极端的预测。

    - 多个解释因子

        数据是由多个潜在解释因子生成的，并且给定每一个因子的状态。

    - 因果因子

        学成表示所描述的变差因素是观察数据x的成因，而并非反过来。

    - 深度

        高级抽象概念能够通过将简单概念层次化来定义。

    - 任务间共享因素

        多个不同变量共享相同的输入x，所以通过共享的中间表表示P（h|x）来学习所有的P(y|x)。

    - 流形

        概率质量集中，并且集中区域是局部连通的，且占据很小的体积。

    - 自然聚类

        输入空间中每个连通流形可以被分配一个单独的类。

    - 时间和空间相干性

        慢特性分析和相关的算法假设，最重要的解释因子随时间的变化很缓慢。

    - 稀疏性

        大部分的特征和大部分的输入不相关。

    - 简化因子依赖

        在良好的高级表示中，因子会通过简单的依赖互相关联。




