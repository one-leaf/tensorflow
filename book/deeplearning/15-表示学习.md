可以将监督学习训练的前馈网络视为表示学习的一种形式。

前馈网络没有对学成的中间特性明确强加条件，其它的表示学习往往会以某种特定的方式明确设计表示。如更多的独立性分布，将表示向量h中元素之间的相互独立的目标函数等。

表示学习提供了无监督学习和半监督学习的一种方法。

1. 贪心逐层无监督预训练

    这个过程是训练一个无监督学习获得输入的分布表示，用于另外的具有相同输入的监督学习任务。

    贪心逐层无监督预训练依赖单层表示学习算法，每一层使用无监督学习预训练，将前一层的输出作为输入，输出数据的新的表示。这个新的表示是更简单的。

    这种贪心逐层训练过程，用于规避监督学习中神经网络难以训练多层的问题。

    贪心逐层训练每次处理一层网络，训练当前层时，保持前面的网络层不变。低层最先训练，并且不会再引入高层网络后再进行调整。当全部完成后，再到监督学习算法中对整个网络进行监督精调。

    1. 何时以及为何无监督预训练有效

        无监督预训练结合了两种不同的想法。第一，利用深度神经网络对初始参数的选择，对模型起到正则化效果。第二，学习输入的分布有助于学习从输入到输出的映射。

        无监督预训练对词向量很有用，因为初始的词向量之间都是等距的。

        无监督预训练对样本标注很少的情况下很有用。

        无监督预训练对学习函数非常复杂的情况下很有用。

        原因是，一，无监督预训练将参数引入到其他方法可能探索不到的领域；二，无监督学习将参数初始化到它们不易逃逸的区域。

        无监督预训练的缺点，一，没有一种明确的方法来调整无监督预训练阶段正则化的强度，其超参数，效果只能事后独立，无法提前预测。二，具有两个独立的训练阶段导致每个训练阶段具有各自的超参数，第二阶段无法在第一阶段中期进行，所以验证需要花费更长的时间。

        如今除了词向量，其他大部分模型都不再进行无监督预训练。因为基于Dropout和批标准化来进行的正则化，可以在很多任务上都超过人类级别的性能。

1. 迁移学习和领域自适应

    迁移学习和领域自适应是利用一个场景中学习到的内容去改善另外一个场景中泛化情况。

    另外不同任务之间共享的有可能不是输入的语义，有可能是输出的语义。例如语音识别系统，网络的输出层是需要识别的音素输出到输出层产生句子，但在输入附近是针对不同的方言产生不同的音素版本。即，如果底层结构是面向任务的，则上层结构是共享的，底层结构学习将面向特定任务的输入转化为通用特征。

    在领域自适应的相关情况下，在每个情景之间任务都是相同，但输入的分布稍有不同。例如针对电影评价的情感分析，转到针对小说的情感分析，由于词汇和风格会因为领域的不同而有差别，这导致跨域的泛化训练很困难。而针对这个情况使用简单的无监督去噪自编码器可以很好的解决。

    还有一个相关的问题是概念漂移，即数据的分布随时间的变化而变化，可以看做一种迁移学习。通常，我们将概念漂移和迁移学习都视为多任务学习的特定形式。

    迁移学习的两种极端形式是一次学习和零次学习，或称为零数据学习。例如如果文本已经很好的描述了猫的形状，即使在没有见过猫的情况下，也能识别图像中的动物是猫。零次学习要求T被表示为某种形式的泛化。

    例如翻译，学习了语言A的分布和语音B的词向量分布，同时学习了语言A翻译到语言B的句子，这样就可以形成单词的翻译。

    多模态学习也是如此，通过学习三组参数(x的分布表示，y的分布表示，以及两个表示之间的关系)，这样一个表示中的概念被锚定在另一个表示中，反之亦然。

1. 半监督解释因果关系

    表示学习的一个重要问题是：什么原因能够使一个表示比另外一个表示更好？理想表示中的特征对应到观测数据的潜在成因，特征空间中不同的特征对应不同的原因，从而表示能够区分这些原因。

    






