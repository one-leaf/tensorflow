目标是寻找神经网络上的一组参数$\theta$，它能显著的降低代价函数$J(\theta)$。该代价函数包括性能评估和正则化项。

1. 学习和纯优化有什么不同

    用于深度模型的优化和传统不同，深度模型不会直接去更新我们关注的性能度量P，只是间接的优化P，通过降低代价函数$J(theta)$来提高P。纯优化是直接最小化目标J本身。

    通常代价函数写为训练集上的平均：

    $$J(\theta)=\mathbb E_{(x,y)\sim\hat p_{data}}L(f(x;\theta),y)$$

    $L$是每个样本的损失函数，$f(x;\theta)$是输入x时所预测的输出，$\hat p_{data}$ 是经验分布，$y$是目标输出。 通常我们更希望的最小化全部的数据分布$p_{data}$，而不是训练集上的经验分布$\hat p_{data}$。

    1. 经验风险最小化

        目标是降低上面所示的期望泛化误差，这个数据量被称为风险。将机器学习问题转化回一个优化问题的最简单办法是最小化训练集上的期望损失。我们叫最小化经验风险。这种训练过程叫经验风险最小化，即：

        $$J(\theta)=\mathbb E_{(x,y)\sim\hat p_{data}}L(f(x;\theta),y)=\frac 1m\sum_{i=1}^mL(f(x_i;\theta),y_i)$$

        m是样本的个数，最小化经验风险很容易过拟合。最有效的还是算法还是基于梯度下降的。深度学习中很少使用经验风险最小化。

    2. 代理损失函数和提前终止

        我们真正关心的损失函数并不能被高效的优化，这种情况下，我们通常会优化代理损失函数。

        例如分类[0,1]的损失是不可解的，我们通常使用负对数似然来代替[0,1]。而且这个效果会比直接学习[0,1]更好，因为即使到了0，继续学习，还能进一步拉开不同类别的距离。

        一般的优化不会停止在局部最小点，以及保持在梯度最小；而机器学习是设计为在发生过拟合之前终止。并且终止时，仍然有较大的导数。

    3. 批量算法和小批量算法

        机器学习和优化算法都是在训练集上完成的，优化一般是使用整个训练集或单个样本，而机器学习都是使用1个以上，但不是全部的样本进行训练称为随机方法。

        小批量的大小由以下因素决定：

        - 更大的批量会计算更精确的梯度估计，但回报却是小于线性的。

        - 极小批量通常难以充分利用多核架构。

        - 如果批量中的所有样本都可以并行处理，硬件设施是批量大小的限制因素。

        - 在某些硬件如GPU上用2的冥数作为批量大小可以获得更少的运行时间。一般都是32~256，16是针对大模型时使用。

        - 可能是小批量在学习中加入了噪声，所以有正则化的效果。小批量学习时需要较小的学习率保持稳定性。

        小批量需要是随机抽取的。

        可以采用并行的计算不同小批量的最小化更新

        小批量如果没有重复的样本都是遵循真实泛化误差，如果第二次遍历，是有偏的。但除非数据量很大，通常好多遍遍历数据集。虽然只有第一遍是无偏估计，但额外的遍历更新会由于减少训练误差得到足够的好处，以抵消带来的训练误差和样本误差之间的差距增加。

        随着数据集的规模迅速增长，很多样本只使用一次，甚至是不完整的使用训练集，这是最的考虑不是过拟合，而是欠拟合和计算效率。

1. 神经网络优化中的挑战

    1. 病态

        在优化凸函数时，最普遍存在的问题就是海森矩阵H的病态。病态问题一般被认为存在于神经网络训练中，病态体现在随机梯度下降会卡在某些情况。此时即使很小的更新步长也会导致代价函数增长。

        判断病态是监控$g^Tg$平方梯度范数和$g^THg$。$g^Tg$平方梯度范数不会显著减少，但$g^THg$会超过一个数量级，导致梯度没有减少，但学习会变得很慢，必需减少学习率。成功训练的神经网络，梯度会显著增加。

        可以用牛顿法来解决病态

    2. 局部极小值

        凸优化的一个突出特点就是为寻找一个局部极小点的问题。

