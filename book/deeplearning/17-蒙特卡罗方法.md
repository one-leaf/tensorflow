随机算法中蒙特卡罗算法可以在任意固定的计算资源下，得到一个近似解。

1. 采样和蒙特卡罗方法

    1. 为什么需要采样

        当我们需要以较小的代价近似许多项的和或某个积分时，采样是一种很灵活的选择。例如小批量训练；另外当我们想训练一个可以从训练分布采样的模型，抽样就是我们的实际目标。

    1. 蒙特卡罗采样的基础

        当无法精确的计算和或积分时，通常可以使用蒙特卡罗采样来近似它。

        蒙特卡罗的思想就是用随机投点法来模拟不规则图形的面积。

        如：在1*1的矩形中，有一个不规则图形，直接计算该图形的面积很困难，那么可以拿N个点，随机抛在矩形内，数一下落入到该不规则图形中的点的个数 count ,那么该不规则图形的面积就可以用 count/N 来近似。

        令：

        $$s=\sum_sp(x)f(x)=E_p[f(x)]$$

        或：

        $$s=\int p(x)f(x)dx=E_p[f(x)]$$

        p是一个关于随机变量x的概率分布（求和时）或概率密度函数（求积分时）

        可以从p中抽取n个样本$x_1$,...,$x_n$来近似s并得到一个经验平均值：

        $$\hat s_n=\frac {1}{n}\sum_{i=1}^nf(x_i)$$

        由下可知$\hat s$是无偏的：

        $$\mathbb E[\hat s_n]=\frac1n\sum_{i=1}^n\mathbb E[f(x_i)]=\frac1n\sum_{i=1}^ns=s $$

        加上根据大数定律，如果样本$x_i$是独立同分布的，那么其平均值将收敛为期望值，即：

        $$\lim_{n\to \infty}\hat s_n=s$$

        则：

        只要满足各个单项的方差有界，即当n增大时，$\hat s_n$的方差只要满足 $Var[f(x_i)]<\infty$,那么，方差$Var[\hat s_n]$就会减小收敛到0：

        $$Var[\hat s_n]=\frac1{n^2}\sum_{i=1}^nVar[f(x_i)]=\frac{Var[f(x)]}{n}$$

        结论，蒙特卡罗估计的期望误差计算为：先计算出$f(x_i)$的经验均值和方差，然后将估计的方差除以样本数n，得到$Var[\hat s_n]$的估计。

        根据中心极限定理，$\hat s_n$的分布收敛到以s为均值和以$\frac{Var[f(x)]}{n}$为方差的正态分布，这样就可以利用正态分布的累积函数来估计$\hat s_n$的置信区间。

        上述是依赖于可以从p(x)的分布均匀采样，如果无法从p采样时，一种办法是利用重要采样，一种办法是构建一个收敛到目标分布的估计序列，即马尔科夫链蒙特卡洛方法。


1. 重要采样

    相对于直接采样，直接采样就是通过对均匀分布采用，实现对任意分布的采样。
    
    重要采样的主要应用就是用一个新的带权重的采样分布来代替原有的采样分布使得采样更加容易高效。

    蒙特卡罗采样即：

    $$E[\hat s]=\int _xp(x)f(x)dx=\frac {1}{n}\sum_{i=1}^nf(x_i)$$

    正常我们会按照 p(x) 的分布来产生随机数进行采样，但如果p(x) 未知，就无法进行针对 p(x) 进行采样来估计期望值。

    所以，我们引入一个新的已知分布 q(x) ，将原来公式变为：

    $$E[\hat s]=\int _xq(x)(\frac {p(x)}{q(x)}f(x))dx$$

    这样我们就可以针对 q(x) 来对 p(x)/q(x)*f(x) 来进行采样。

    $$E[\hat s_q]=\frac {1}{n} \sum _{i=1,x_i\sim q}^n \frac {p(x_i)}{q(x_i)}f(x_i)$$

    然后，q(x)可以通过计算相差距离或其它评估指标被简单的推导出来。

    如果要保证方差最小，还需要使得q(x)的和或积分为1，即：

    $$q(x)=\frac {p(x)|f(x)|}{Z}$$

    Z为归一化常数。

    问题是高维度下q很难确定。

    对应机器学习的分类器，其中代价函数的大部分代价来自少量的错误分类的样本，这种情况下，通过更频繁的抽取这些困难样本可以减少梯度估计的方差。

1. 马尔可夫链蒙特卡罗方法

    很多情况下，希望采用蒙特卡罗方法，但又不存在一种简单的方法可以直接从目标分布中精确采样或者一个方差小的重要采样，这时可以采用利用马尔可夫链来进行蒙特卡罗估计。

    马尔可夫链蒙特卡罗方法(MCMC)最常用的基于能量的模型，即：$p(x)\propto exp(-E(x))$。

    马尔可夫链的核心思想是从某个可取任意值的状态x出发。随着时间的推移，我们随机的反复更新状态x，最终x成为了一个从p(x)中抽出的非常接近一般的样本。定义中，马尔可夫链有一个随机状态x和一个转移分布$T(x^{'}|x)$定义而成。$T(x^{'}|x)$是一个概率分布，定义了给定状态x的情况下随机的转移到$x^{'}$的概率。这样如果并行运行多个马尔可夫链，就可以完成目标$q_t(x)$收敛到$p(x)$。

    无论状态是连续的还是离散的，所有的马尔可夫链方法都包含重复、随机的更新直到最后状态开始从均衡分布中采样。运行马尔可夫链直到它达到均衡分布的过程通常称为马尔可夫链的磨合过程。达到平衡后，可以从均衡分布中抽取一个无限多数量的样本序列，这些样本服从同一分布，但两个连续的样本之间会高度相关，所以一般是每隔n个样本返回一个样本。

    马尔可夫链的问题是计算成本太高，另外无法预知磨合时间需要多长，也无法知道磨合是否已经达到平衡，只能用启发式的方法来检查和判读是否磨合成功。

    [参考代码](./code/17-2.py)

1. Gibbs 采样

    Gibbs采样是一种特殊的马尔可夫链算法，常被用于解决包括矩阵分解、张量分解等在内的一系列问题，也被称为交替条件采样（alternating conditional sampling），其中，“交替”一词是指Gibbs采样是一种迭代算法，并且相应的变量会在迭代的过程中交替使用，除此之外，加上“条件”一词是因为Gibbs采样的核心是贝叶斯理论，围绕先验知识和观测数据，以观测值作为条件从而推断出后验分布。

    Gibbs采样适用于条件分布比边缘分布更容易采样的多变量分布。

    假设，需要从一个联合分布 $q(x_i,...,x_n)$ 中抽取 $X =(x_1,...,x_n)$ 的k个样本。记第i个样本为$X^i=(x_1^i,...,x_n^i)$，则采样过程为：

    1. 确定初始值 $X^1$

    2. 假设已经得到样本 $X^i$, 则下一个样本为 $X^{x+1}=(x_1^{i+1},...,x_n^{i+1})$。 于是可以将其看作为一个向量，可以通过在其他分量已知的条件下用该分量的概率分布来抽取该分量。对于此条件概率，我们使用样本 $X^{x+1}$ 中已得到的分量$x_i^{i+1}$ 到 $x_{i-1}^{i+1}$ 以及上一个样本 $X^i$中的分量$x_{j+1}^i$到$x_n^i$，即$p(x_i^{i+1}|x_1^{i+1},...,x_{j-1}^{x+1},x_{j+1}^i,...,x_n^i)$。

    3. 重复上述过程 k 次

    在采样完成后，我们可以用这些样本来近似所有变量的联合分布。如果仅考虑其中部分变量，则可以得到这些变量的边缘分布。此外，我们还可以对所有样本求某一变量的平均值来估计该变量的期望。

    [参考代码](./code/17-1.py)

1. 不同的峰值之间的混合挑战

    采样MCMC方法(马尔可夫链蒙特卡罗方法)的主要难点在于马尔可夫链的混合通常不理想，从设计好的马尔可夫链中采样连续样本不是完全独立的，并且会出现按概率大小访问许多不同的区域。

    我们将高维情况下采样的结果具有很强相关性的现象称为慢混合或混合失败。具有缓慢混合的MCMC方法可以视为对能量函数无意的执行类似于带噪声的梯度下降的操作。尤其是采样Gibbs采样算法时，会在一个峰值附近抽取远超过需求样本的现象更为明显。两个峰值直接如果存在巨大的低概率区域则在两个峰之间转移的概率呈指数下降。尤其是Gibbs采样的每一步都是只更新变量的一小部分，而这一小部分变量又严重依赖其他的变量时，就会出现问题。

    1. 不同峰值之间通过回火来混合

        之前定义能量模型为：

        $$p(x)=e^{-E(x)}$$

        现在改为：

        $$p_\beta(x)=e^{-\beta E(x)}$$

        $\beta$ 参数可以理解为温度的倒数，当温度趋近为0时， $\beta$无穷大，这时模型是确定的，如果温度趋近于无穷大时，$\beta$趋近为0，这样模型就成了均匀分布。

        这样可以利用温度，尤其是利用 $\beta<1$ 的情况实现在不同峰值之间快速混合，这种策略称为回火。

        但即使采样这样，也难以应用到采样复杂的能量模型，一个可能的原因是在临界温度时，温度下降必须设置的非常慢，才能保证回火的有效性。

    1. 深度也许会有助于混合

        实际发现堆叠越深的正则化自编码器，其顶端空间的边缘分布越趋向于均匀和发散，而且不同峰值所对应的区域之间的间距也会越小。 





