1. 上溢和下溢

    $$softmax(x)_i=\frac {e^{x_i}}{\sum _{j=1}^ne^{x_j}}$$ 
    
    如果 $x_j$的值很小的负数，则$e^{x_j}$为0，则分母为0，导致值无意义；如果$x_i$为大的正数时，$e^{x_i}$ 趋于无穷大，超界。

    这两个问题可以采用 softmax(z) 解决，$z=x-max_ix_i$ 

    >例如计算 [3,1,-3] 中间 1 的softmax:
    >
    >传统方法：
    >
    >$$ softmax(1) = \frac {e^1}{e^3+e^1+e^{-3}} = \frac {2.7}{20+2.7+0.05} = 0.12 $$
    >
    >新方法：
    >    
    >$$ M = max([3,1,-3]) = 3 $$
    >
    >$$ softmax(1) = \frac {e^{1-M}}{e^{3-M}+e^{1-M}+e^{-3-M}} $$
    >$$ = \frac {e^{1-3}}{e^{3-3}+e^{1-3}+e^{-3-3}} = \frac {0.1353}{1+0.1353+0.0025} = 0.12 $$

    ```python
    import numpy as np

    m = np.array([3,1,-3])
    e_m = np.exp(m - np.max(m))
    e_m / e_m.sum()
    ```

    以上分子最大0，分母最小1，所以同时解决了上溢和下溢的问题。

    下一步求 log(softmax(x)) ，softmax(x) 有可能为0，导致 log(0) 无意义。 

    解决办法：

    $$\begin{aligned} log[f(x_i)]&=log(\frac{e^{x_i}}{e^{x_1}+e^{x_2}+...e^{x_n}})  \\\\
    &=log(\frac{ \frac{e^{x_i}}{e^M} }{ \frac{e^{x_1}}{e^M}+\frac{e^{x_2}}{e^M}+...\frac{e^{x_n}}{e^M} })  \\\\
    &=log(\frac {e^{(x_i-M)}} {\sum _j^ne^{(x_j-M)} }) \\\\
    &=log({e^{(x_i-M)}}) - log(\sum _j^ne^{(x_j-M)}) \\\\
    &= (x_i-M)-log(\sum _j^ne^{(x_j-M)}) \end{aligned}$$

    可以看到log的求和项最小为1，就解决了log项的下溢问题。

1. 病态条件

    条件数是函数相对于输入的微小变化而变化的快慢程度。输入轻微而函数变化剧烈会有问题。

    病态条件的矩阵会放大预先存在的错误，这个错误将会和反向算法的数值误差进一步复合。

1. 基于梯度的优化方法

    优化方法是改变x以最小化或最大化f(x)的任务，一般都是最小化f(x),最大化可以采用-f(x)来实现。

    需要最小化函数称为目标函数或准则。但最小化时，也称为代价函数、损失函数或误差函数。

    一般用上标 * 表示最小化函数，例如：$x^*=argmin\ f(x)$

    - 微积分与优化

        - 定义 $y=f(x)$,其中x、y都是实数，这个函数的导数为 $f'(x)$ 或 $\frac {d_y}{d_x}$ , 导数 f‘(x) 代表 f(x) 在 x 点上的斜率。

        - 导数对最小化的用处在于，告诉我们如何更改x来略微的改善y。我们将x往导数的反方向移动一小步来减小f(x),这种技术成为梯度下降。

        - 当导数 f'(x)=0 时，导数无法提供移动信息，这个点称为临界点或驻点。一个局部极大点说明不管如何移动，f(x)都比所有临近点大。有些临界点不是最小点也不是最大点，称为鞍点。

        - f(x) 绝对的最小值的点，称为全局最小点。当出现多个局部极小点或平坦区域时，优化函数将有可能无法找到全剧最小点。

        - 如果有多维输入的情况，则采用偏导数。偏导数$\frac {\partial }{\partial x_{i}}f(x)$ 衡量点x处只有$x_i$增加时f(x)如何变化。梯度是相对一个向量求导的导数，f的导数是包含所有偏导数的向量，记做：$\nabla _xf(x)$, 梯度的第i个元素是f关于$x_i$的偏导数。在多维情况下，临界点是梯度中所有元素都为0的点。

        - 在u(单位向量)方向的方向导数，是函数f在u方向的斜率。方向导数是函数 $f(x+\alpha u)$关于$\alpha$的导数（在$\alpha=0$时取得）。根据链式法则，当$\alpha=0$时， $\frac {\partial }{\partial x_{i}}f(x+\alpha u)=u^T\nabla _xf(x)$, 为了最小化f，找到使f下降最快的方向，计算方向导数：

            $$ \min _{u,u^Tu=1} u^T\nabla _xf(x) $$
            $$ = \min _{u,u^Tu=1} ||u||_2||\nabla _xf(x)||_2cos\theta $$

            $\theta$是u与梯度的夹角，将$||u||_2=1$代入，可以简化为 $\min _{u} cos\theta$

        - 我们可以在负梯度上移动可以减小f，这个称为最速下降法或梯度下降。

        - 最速下降的新的点为 $x'=x-\varepsilon \nabla _xf(x)$,其中$\varepsilon$ 为学习率，是一个确定步长大小的正标量。这个值通常是一个小参数，可以通过实际测试能产生最小化目标函数来找到，这个策略称为线搜索。

        - 最速下降到每一个元素都为0或接近为0时，可以尝试直接解方程 $\nabla _xf(x)=0$得到临界点。

    - 梯度之上：雅可比和海森矩阵

        所有输入和输出都为向量的函数的偏导数矩阵称为雅可比矩阵(Jacobian)。

        定义：$f:\ \mathbb{R}^m \rightarrow \mathbb{R}^n$ 

        则雅可比矩阵J为： $J \in \mathbb{R}^{nxm}$ ,$J_{i,j}=\frac {\partial}{\partial x_j}f(x)_i$

        导数的导数称为二阶导数

        一阶导数是衡量输入导致的变化率，二阶导数是衡量曲率。如果曲率为0，则代价函数符合预期的下降速度；如果是负曲率，则代价函数实际上比预测的下降的更快；正曲率，代价函数比预计的下降更慢，并且最终会增加，这时如果太大的步骤会导致增加函数值。

        如果是多维的情况，所有的维度的二阶导数组成的矩阵，称为海森矩阵(Hessian)。

        定义： $H(f)(x)_{i,j}=\frac {\partial^2}{\partial x_i\partial x_j}f(x)$

        海森矩阵等于梯度的雅可比矩阵。

        可以通过二阶导数预期梯度下降步骤的表现和计算最优步长。还可以用二阶导数确定一个临界点是否是局部最大点、局部最小点或鞍点。当二阶导数特征值全部为正，则为局部最小点；全部为负，则为局部最大点；除外为鞍点。

        如果梯度下降表现很差时，可以利用牛顿法，利用海森矩阵来指导下降。

        牛顿法先假设任务是优化一个目标函数f，求函数的极小问题，可以转换为求解函数f的导数 f'=0 的问题，如下：

        泰勒级数为：

        $$g(x)=1+x+\frac {x^2}{2!}+\frac {x^3}{3!}+...$$

        函数f(x)在a处的多项式展开，即泰勒展开式为：

        $$f(x)=\sum_{n=0}^N\frac{f^{(n)}(a)}{n!}(x-a)^n+R_n(x)$$
        
        $R_n(x)$是$(x-a)^n$的高阶无穷小，省略掉，为了求解 f'=0 的根，利用泰勒公式把f(x)在$x_n$处展开到二阶，即：

        $$f(x)\approx f(x_n)+f'(x_0)(x-x_n)+\frac {f''(x_n)}{2}(x-x_n)^2$$

        然后用f(x)的最小点作为新的探索点$x_{n+1}$，据此，令：

        $$ f'(x) = f'(x_n) + f''(x_n)(x-x_n) = 0 $$

        求出迭代公式，即：

        $$x_{n+1}=x_n-\frac {f'(x_n)}{f''(x_n)},\ n=0,1,...$$

        高维度的牛顿迭代公式为：

        $$x_{n+1}=x_n-Hf(x_n)^{-1}\nabla f(x_n),\ n \geq 0 $$

        一般认为牛顿法比梯度下降法利用了曲率信息，更容易收敛。

        仅仅利用梯度信息的优化算法称为一阶优化算法，例如梯度下降；使用海森矩阵的优化算法称为二阶最优化算法，如牛顿法。

        在限定领域最成功的优化为凸优化，但只对凸函数有效。但深度学习中的很难表示成凸优化的形式，凸优化只能作为深度学习算法中的子算法。目前深度学习中的各种优化算法，由于使用的函数簇非常复杂，所以大多数缺乏理论保证。但限制函数利普希茨(Lipschitz)连续或其导数利普希茨连续，可以获得一些保证。利普希茨连续要求函数曲线上的任意两点的斜率一致有界，就是任意斜率都小于同一个常数，这个常数称为利普希茨常数。这个属性可以量化训练假设，即梯度下降算法产生的输入的微小变化导致输出也是微小变化。反例 $f(x) = \sqrt {x}$这个函数两点间的斜率可以无限大，因此不是利普希茨连续。

1. 约束优化

    希望在x的某些集合$\mathbb {S}$中找到f(x)的最大值或最小值，这个称为约束优化。集合$\mathbb {S}$中的点x称为可行点。例如对 x 加一个范数约束 $||x|| \leq 1$。

    简单的做法修改梯度下降的步长或将线上的每一个点投影到约束区域。

    或设计一个不同的、无约束的优化问题，其解可以转化为原始约束的解。如：求解有x约束为L2范数的解，可以转换为求解最小化 $\theta$, $\theta = f(|cos\theta,sin\theta|^T)$ ，最后返回 $[cos\theta,sin\theta]$ 做为原始 L2 范数约束的解。 

    还可以使用卡罗需-库恩-塔克条件（KKT）来求解。

    假设目标函数$f:\mathbb {R}^n\rightarrow \mathbb {R}$及约束函数$g_i:\mathbb {R}^n\rightarrow \mathbb {R}$皆为凸函数，而$h_i:\mathbb {R}^n\rightarrow \mathbb {R}$是一仿射函数，假设有一可行点$x^*$，如果有常数$\mu _i \geq 0(i=1,...m)$及$v_j(j=1,...,l)$令到：

    $$\nabla _xf(x^*)+\sum _{i=1}^m\mu _i\nabla g_i(x^*)+\sum _{j=1}^lv_j \nabla h_j(x^*)=0$$
    $$\mu _ig_i(x^*)=0\ for\ all\ i=1,...,m,$$
    那么$x^*$这点就是全局极小值。

1. 实例：线性最小乘

    假设，我们找到下面x的最小化值：

    $$f(x)=\frac {1}{2}||Ax-b||_2^2$$

    - 定义：

        $$||x||_2^2=\sqrt {(\sum _{i=1}^mx_i^2)}=\sqrt {x^Tx}$$

    - 求解：

        $$\begin{aligned}f(x)&=\frac {1}{2}(Ax-b)^T(Ax-b) \\\\
        &=\frac {1}{2}(x^TA^T-b^T)(Ax-b) \\\\
        &=\frac {1}{2}(x^TA^TAx-2b^TAx+b^Tb)\end{aligned}$$
        注：b、x都是列向量，所以$b^TAx$是标量，标量的转置等于自身，即：$b^TAx=X^TA^Tb$

    - 对x求导，得到梯度：

        $$\nabla _x f(x)=A^TAx-A^Tb=A^T(Ax-b)$$

    - 最小二乘法计算：
        >将步长($\varepsilon$)和容差($\delta$)设置为小的正数
        >
        > $while\ ||A^TAx-A^Tb||_2>\delta$
        >
        > $\qquad x\leftarrow - \varepsilon(A^TAx-A^Tb)$

1. 详细实例，最小二乘法：

    用3个温度计测量当前温度，求平均温度：
    |  | 温度计1 | 温度计2 | 温度计3 |
    | ------ | ------ | ------ | ------ |
    | 温度 | 29.7 | 28.7 | 30.1 |

    常规解法，求平均值 $\overline C = \frac {(C1+C2+C3)}{3} = 29.5$

    最小二乘解法：

    $$S=\sum _{i=1}^3(C-C_i)^2$$

    S为种误差值，我们需要最小化S，求C。两边求导：

    $$\begin{aligned}\frac {dS}{dC}=0&=\frac {d\sum _{i=1}^3(C-C_i)^2}{dC} \\\\
    &=\frac {d\sum _{i=1}^3(C^2-2CC_i+C_i^2)}{dC} \\\\
    &= \sum_{i=1}^32C-2C_i \\\\
    &= 2\sum_{i=1}^3(C-C_i) \\\\
    &= 2((C-C_1)+(C-C_2)+(C-C_3)) \\\\
    &= 2(3C-(C_1+C_2+C_3))\end{aligned}$$
    
    求解得：

    $$ 2(3C-(C_1+C_2+C_3))=0 $$
    $$ C = \frac {C_1+C_2+C_3}{3}=29.5$$

    已知温度和冰淇淋的销量，预测某一温度下销量：
    | 温度 | 31 | 33 | 35 |
    | ------ | ------ | ------ | ------ |
    | 销量 | 113 | 119 | 123 |

    看上去是线性关系，先假定一个线性方程来匹配：

    $$f(x)=ax+b$$

    定义误差函数为：

    $S=\sum _{i=1}^3(ax_i+b-y_i)^2$

    $y_i$是同一温度下预测的值。x是温度，ax+b是预测销售值。目标是最小化S

    由于有2个未知数a、b，分别求偏导：

    $$\begin{aligned}
    \frac {\partial S}{\partial a}&=\frac {d\sum((ax_i)^2+2ax_i(b-y_i)+(b-y_i)^2)}{da}  \\\\
    &=\sum 2ax_i^2+2x_i(b-y_i) \\\\
    &=2\sum(ax_i+b-y_i)x_i=0  
    \end{aligned}$$

    $$\begin{aligned}
    \frac {\partial S}{\partial b}&=\frac {d\sum(ax_i-y_i+b)^2}{db} \\\\
    &=\frac {d\sum((ax_i-y_i)^2+2(ax_i-y_i)b+b^2)}{db} \\\\
    &=\sum (2(axi-y_i)+2b) \\\\
    &=2\sum(ax_i+b-y_i)=0  
    \end{aligned}$$

    列方程为：

    $$\begin{cases}
    2((31a+b+113)*31+(33a+b+119)*33+(35a+b+123)*35)=0\\\\
    2((31a+b-113)+(33a+b-119)+(35a+b-123))=0
    \end{cases}$$

    解方程得：

    $$y=2.5x+35.83$$

    我们还可以假设方程为如下方程：

    $$f(x)=ax^2+bx+c$$

    或

    $$f(x)=a/x+b$$

    或

    $$f(x)=ae^{(x+b)}$$
    
    也一样可以做线性二乘法。

    最小二乘法求解的值满足误差的正态分布。如果某些场合不满足正态分布，也可以通过对数据取对数得到正态分布的结果。

1. 正规方程

    除了使用求导可以解一个属性，如果有多属性，可以使用正规方程来求解，即：$f(x_i)=w^Tx_i+b$

    假设我们有m个样本。特征向量的维度为n。

    为了方便，我们将w和b作为同一个向量$\theta$来表示，则为
    
    $$\theta=(b,w_1,...,w_n)=(\theta_0,...,\theta_n)$$

    因此，样本需要为参数 $b=\theta_0$ 增加一个常量为1的维度，即 X 的维度改变为 m * (n+1)     

    $$X=\begin{bmatrix} 
        1 & x_{1,1} & x_{1,2}& \cdots & x_{1,n} \\
        1 & x_{2,1} & x_{2,2}& \cdots & x_{2,n} \\
        \vdots & \vdots &\vdots &\vdots &\vdots \\
        1 & x_{m,1} & x_{m,2}& \cdots & x_{m,n} 
        \end{bmatrix}
    ,y=\begin{bmatrix}y_1\\y_2\\ \vdots \\y_m\end{bmatrix}
    ,\theta= \begin{bmatrix}\theta_0\\\theta_1\\ \vdots \\\theta_n\end{bmatrix}
    $$

    得：

    $$y=X*\theta$$

    已知:
    
    - 单位矩阵E乘以任意矩阵等于矩阵本身：  $AE=EA=A$
        
    - 矩阵的逆乘以矩阵等于单位矩阵： $AA^{-1}=A^{-1}A=E$

    - 矩阵的逆必须是方阵： $A^TA$

    所以变换如下：

    $$X*\theta=y$$
    $$\Rightarrow X^TX*\theta=X^Ty$$
    $$\Rightarrow (X^TX)^{-1}X^TX*\theta=(X^TX)^{-1}X^Ty$$
    $$\Rightarrow E*\theta=(X^TX)^{-1}X^Ty$$
    $$\Rightarrow \theta=(X^TX)^{-1}X^Ty$$

    代码解：

    ```python
    import numpy as np

    m=10; n=5
    w=3; b=5
    x=np.random.random((m, n))
    y=np.sum(x*np.full((m, 1),w),axis=1) + b + np.random.normal(0, 0.02, (m))

    X=np.column_stack((np.ones(m),x))
    np.linalg.inv(np.dot(X.T,X))@X.T@y
    ```

1. 最小二乘法代码

    演示如何采用最小二乘法回归，以及正则项对过拟合的影响：

    [样例代码](./code/04-1.py)
