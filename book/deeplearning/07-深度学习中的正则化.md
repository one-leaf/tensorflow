大部分的正则化都是对估计进行正则化，估计的正则化以偏差的增加换取方差的减少。正则化的目标是将除了包括真实的数据生成过程，还包括了许多其他可能的生成过程--方差（而不是偏差）造成的过拟合，改为只包括真实的数据生成过程。

1. 参数范数惩罚

    对目标函数 J 添加一个参数范数惩罚 $\Omega(\theta)$， 限制模型的的学习能力。最终的目标函数为：

    $$\hat J(\theta;X,y)=J(\theta;X,y)+\alpha\Omega(\theta)$$

    当训练 $\hat J$ 时，$\alpha\Omega(\theta)$ 会降低原始目标 $J$ 关于训练集的误差并同时减小参数 $\theta$ 的规模。

    另外参数包括了权重和偏置，但正则化只对权重进行惩罚，因为偏置的共享较少，对方差影响不大，而且如果惩罚偏置会造成难拟合。

    有些升级网络会分别对每一层单独设置正则化权重衰减参数，但调参困难，同时是全局共用一个权重衰减参数。

    1. L2参数正则化