大部分的正则化都是对估计进行正则化，估计的正则化以偏差的增加换取方差的减少。正则化的目标是将除了包括真实的数据生成过程，还包括了许多其他可能的生成过程--方差（而不是偏差）造成的过拟合，改为只包括真实的数据生成过程。

1. 参数范数惩罚

    对目标函数 J 添加一个参数范数惩罚 $\Omega(\theta)$， 限制模型的的学习能力。最终的目标函数为：

    $$\hat J(\theta;X,y)=J(\theta;X,y)+\alpha\Omega(\theta)$$

    当训练 $\hat J$ 时，$\alpha\Omega(\theta)$ 会降低原始目标 $J$ 关于训练集的误差并同时减小参数 $\theta$ 的规模。

    另外参数包括了权重和偏置，但正则化只对权重进行惩罚，因为偏置的共享较少，对方差影响不大，而且如果惩罚偏置会造成难拟合。

    有些升级网络会分别对每一层单独设置正则化权重衰减参数，但调参困难，同时是全局共用一个权重衰减参数。

    1. L2参数正则化

        最简单的和常用的参数范数惩罚是权重衰减的L2参数范数惩罚，通过添加 $\Omega(\theta)=\frac 12||w||_2^2$ ,使权重更接近原点。L2也称为岭回归。

        假设没有偏置b，目标函数为：

        $$\hat J(w;X,y)=\frac {a}{2}w^Tw+J(w;X,y)$$

        对应的梯度为：

        $$\nabla_w\hat J(w;X,y)=aw+\nabla_wJ(w;X,y)$$

        使用单步梯度下降更新权重，即：

        $$w\leftarrow w-\in (aw+\nabla_wJ(w;X,y))$$

        $$w\leftarrow (1-\in a)w-\in\nabla_wJ(w;X,y)$$

        先看不加正则项的情况，令 $w_0$ 为未正则化之前的w，则损失近似为：

        $$\hat J(\theta)=J(w_0)+\frac 12(w-w_0)^TH(w-w_0)$$

        H是J在$w_0$处计算的海森矩阵，当$\hat y$为最小值时，$w_0$最优，梯度为0，得J梯度为：

        $$\nabla_w\hat J(w)=H(w-w_0)=0$$    

        如果加上正则项，等式为：

        $$aw+H(w-w_0)=0$$

        推导：

        $$w=(H+aI)^{-1}Hw_0$$

        对H进行特征分解得（A是对角矩阵，Q是特征矩阵的标准正交基，$H=QAQ^T$）：

        $$w=(QAQ^T+aI)^{-1}QAQ^Tw_0$$
        $$=(QAQ^TQ+aIQ)^{-1}AQ^Tw_0$$
        $$=Q(A+aI)^{-1}AQ^Tw_0$$

        得出：

        按 $\frac \lambda{\lambda+a}$ 缩放H，所以当 $\lambda>>a$ 时，基本不受影响；当$\lambda<<a$时，影响很大，会严重收缩。这样就相当于“降噪”，对特征值大的不影响，对特征值小的减少到0，从而起到降低过拟合的作用。

    1. L1正则化

        对w的L1正则化,即绝对值和最小，定义为

        $$\Omega(\theta)=||w||_1=\sum_i|w_i|$$

        正则化后的目标函数为：

        $$\hat J(w;X,y)=a||w||_1+J(w;X,y)$$

        求梯度为(sign(w)为符号函数，正数为1，负数为-1)：

        $$\nabla_w\hat J(w;X,y)=asign(w)+\nabla_w J(w;X,y)$$

        因为引入了符号函数，将不是线性的缩放w，所以分开区间讨论：当 w >0 时，梯度会得到增加，而当 w<0 时，梯度会被降低，这样最终的结果会导致w不管特征值的大小，全部趋近于0，造成w稀疏。

        这种效果被用于特征选择机制，配合最大似然，可以有效去除无效的特征。

    1. 作为约束的范数惩罚

        将通用的参数正则化代价函数

        $$\hat J(\theta;X,y)=J(\theta;X,y)+\alpha\Omega(\theta)$$

        转换为拉格朗日乘数法：

        $$\mathcal{L}(\theta,a;X,y)=J(\theta;X,y)+ a(\Omega(\theta)-k)$$ 

        求解：

        $$\hat \theta=arg\min_\theta \max_{a,a\geq 0}\mathcal{L}(\theta,a)$$

        可以观察到，当$\Omega(\theta)>k$时是增量，当$\Omega(\theta)<k$是减少。所有正值的a都在减小$\Omega(\theta)$。但即使a为最优值时，也不会导致 $\Omega(\theta)<k$ 。

        为了方便观察，将 a 固定到 a 的最优值 $\hat a$：

        $$\hat \theta=arg\min_\theta \mathcal{L}(\theta,\hat a)=arg\min_\theta J(\theta;X,y)+\hat a\Omega(\theta)$$

        这个表达式和正则化权重一致，所以可以将参数的范数惩罚看做是对权重的约束。

        相当于将参数约束到一个a的函数区间。增大a就会得到小的约束区域，减少a会得到一个大的约束区域。

        当然也可以通过修改梯度下降算法来显式的对w进行约束。

        不直接用正则化约束的原因是会导致目标函数非凸，有陷入局部极小的问题。

        其他推荐的策略是，约束神经网络层权重矩阵的每列的范数，而不是直接限制整个权重矩阵的F范数。分别限制每一列的范数，可以防止某一隐藏单元有非常大的权重。（矩阵的1范数是基于列的，F范数是整个的，2范数是最大特征值）

        [参考阅读](https://zhuanlan.zhihu.com/p/29360425)

    1. 正则化和欠约束问题

        由于机器学习中大量依赖$X^TX$求逆，如果样本较少，某一个维度没有观察到方差，就会出现奇异矩阵，而正则化会让这个奇异矩阵可逆$X^TX+aI$（实际上w的初始化都是有值的，所以这个没用）。另外如果一个向量w可以完美分类会导致2w也完美分类，最终梯度下降会不断增加w的值，知道溢出，而正则化会阻止这个情况。

        总之，正则化有助于稳定欠定定向题。

    1. 数据增强

        要泛化更好的最好办法是增加样本，由于样本总是有限，所以可以创造假数据插入到样本中。

        增加样本的时候，不能增加会改变类别的转换，如，b和d，6和9的识别，这种情况下水平翻转或旋转180度，就不适合。

        在神经网络的输入层引入噪声也是数据增强的一种。也可以直接向隐藏层输入噪声。

    1. 噪声鲁棒性

        
