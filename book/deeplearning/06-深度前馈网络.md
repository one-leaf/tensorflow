深度前馈网络（多层感知机）是将输入的x映射到y，前馈网络定义了一个映射$y=f(x;\theta)$，并且学习参数$\theta$的值，使其得到最佳的函数近似。

这种是前向的，没有将输出的y重新加入到模型，也就是输出和模型之间没有反馈连接。区别于循环神经网络。

称为网络的原因是将很多不同的函数复合在一起，例如$y=f_3(f_2(f_1(x)))$，这种链式结构是神经网络中最常用的结构。链的全长称为模型的深度。前馈神经网络的最后一层称为输出层。中间层由于训练数据并没有给定每一层的输出，所以也称为隐藏层。隐藏层的维度决定了模型的宽度。可以将层想象为许多并行的单元组成，每个单元对应向量的节点称为神经元。神经元输入接收其他的神经元输出，并计算自己的激活值。

线性模型的好处是对凸函数能够高效可靠的拟合，但无法理解两个输入变量之间的相互作用。有一种技巧是对输入x做非线性处理后再用线性模型去拟合，例如使用核技巧。函数表示为 $y=f(x;\theta,w)=\phi(x;\theta)^Tw$，这样就有两个任务，一个是学习$\phi(x;\theta)$ 中的$\theta$；一个是$f(x';w)$中的$w$，其中$x'$是$\phi(x)$的映射。这样我们可以将已知的知识编码进网络来帮助泛化，所以只要找到一个非常广泛的函数族$\phi(x;\theta)$就可以改善线性模型的的泛化。

1. 实例：学习XOR

    XOR运算，就是（0，0）= 0；（0，1）= 1；（1，0）= 1；（1，1） = 0；

    模型采用线性模型：

    $$f(y)=x^Tw+b$$

    损失函数采用均方误差，X为4组值，所以平均数除以4：

    $$J(\theta)=\frac 14\sum (f(x)-f(x;\theta))^2$$

    尝试直接用正规方程解，方程如下：

    $$\theta=(X^TX)^{-1}X^Ty$$

    $$X=\begin{bmatrix} 1 & x_{1,1} & \cdots & x_{1,n} 
    \\ 1 &x_{2,1} &\cdots & x_{2,n}
    \\ \vdots & \vdots & \vdots & \vdots
    \\ 1 & x_{m,1} &\cdots &x_{m,n}
    \end{bmatrix} = \begin{bmatrix} 1&0&0\\1&0&1\\1&1&0\\1&1&1  \end{bmatrix}$$

    $$y=\begin{bmatrix} 0\\1\\1\\0  \end{bmatrix}$$

    求解带入正规方程解得：

    $$\theta = \begin{bmatrix} 0.5\\0\\0 \end{bmatrix} \Rightarrow b=0.5;w=w1=w2=0$$

    但这个解造成的y的输出全部是0.5，这不是我们希望的，所以引入非线性特性：

    我们再增加一个隐藏层(W,c)，并引入非线性激活函数 ReLU，如下：

    $$f(x:W,c,w,b)=w^T\max(0,W^Tx+c)+b$$

    乱猜求解得：

    $$W=\begin{bmatrix} 1&1\\1&1 \end{bmatrix};
    c=\begin{bmatrix} 0&-1 \end{bmatrix};
    w=\begin{bmatrix} 1\\-2 \end{bmatrix};
    b=\begin{bmatrix} 0 \end{bmatrix}$$

    程序代码，用梯度下降法求解：

    [演示程序](../06-1.py)

    

    



    




