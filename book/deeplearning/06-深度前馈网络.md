深度前馈网络（多层感知机）是将输入的x映射到y，前馈网络定义了一个映射$y=f(x;\theta)$，并且学习参数$\theta$的值，使其得到最佳的函数近似。

这种是前向的，没有将输出的y重新加入到模型，也就是输出和模型之间没有反馈连接。区别于循环神经网络。

称为网络的原因是将很多不同的函数复合在一起，例如$y=f_3(f_2(f_1(x)))$，这种链式结构是神经网络中最常用的结构。链的全长称为模型的深度。前馈神经网络的最后一层称为输出层。中间层由于训练数据并没有给定每一层的输出，所以也称为隐藏层。隐藏层的维度决定了模型的宽度。可以将层想象为许多并行的单元组成，每个单元对应向量的节点称为神经元。神经元输入接收其他的神经元输出，并计算自己的激活值。

线性模型的好处是对凸函数能够高效可靠的拟合，但无法理解两个输入变量之间的相互作用。有一种技巧是对输入x做非线性处理后再用线性模型去拟合，例如使用核技巧。函数表示为 $y=f(x;\theta,w)=\phi(x;\theta)^Tw$，这样就有两个任务，一个是学习$\phi(x;\theta)$ 中的$\theta$；一个是$f(x';w)$中的$w$，其中$x'$是$\phi(x)$的映射。这样我们可以将已知的知识编码进网络来帮助泛化，所以只要找到一个非常广泛的函数族$\phi(x;\theta)$就可以改善线性模型的的泛化。

1. 实例：学习XOR

    XOR运算，就是（0，0）= 0；（0，1）= 1；（1，0）= 1；（1，1） = 0；

    模型采用线性模型：

    $$f(y)=x^Tw+b$$

    损失函数采用均方误差，X为4组值，所以平均数除以4：

    $$J(\theta)=\frac 14\sum (f(x)-f(x;\theta))^2$$

    尝试直接用正规方程解，方程如下：

    $$\theta=(X^TX)^{-1}X^Ty$$

    $$X=\begin{bmatrix} 1 & x_{1,1} & \cdots & x_{1,n} 
    \\ 1 &x_{2,1} &\cdots & x_{2,n}
    \\ \vdots & \vdots & \vdots & \vdots
    \\ 1 & x_{m,1} &\cdots &x_{m,n}
    \end{bmatrix} = \begin{bmatrix} 1&0&0\\1&0&1\\1&1&0\\1&1&1  \end{bmatrix}$$

    $$y=\begin{bmatrix} 0\\1\\1\\0  \end{bmatrix}$$

    求解带入正规方程解得：

    $$\theta = \begin{bmatrix} 0.5\\0\\0 \end{bmatrix} \Rightarrow b=0.5;w=w1=w2=0$$

    但这个解造成的y的输出全部是0.5，这不是我们希望的，所以引入非线性特性：

    我们再增加一个隐藏层(W,c)，并引入非线性激活函数 ReLU，如下：

    $$f(x:W,c,w,b)=w^T\max(0,W^Tx+c)+b$$

    乱猜求解得：

    $$W=\begin{bmatrix} 1&1\\1&1 \end{bmatrix};
    c=\begin{bmatrix} 0&-1 \end{bmatrix};
    w=\begin{bmatrix} 1\\-2 \end{bmatrix};
    b=\begin{bmatrix} 0 \end{bmatrix}$$

    程序代码，用梯度下降法求解：

    [演示程序](../06-1.py)

2. 基于梯度的学习

    由于非线性的引入，导致神经网络的训练只能采用迭代方式，基于梯度的优化策略，而不能先线性模型那样直接针对方程求解。
    
    凸优化可以从任何一种初始化参数值都会收敛，但用于非凸优化的针对损失函数的随机梯度下降没有这种收敛性保证，而且对参数值很敏感。所以通常将权重初始化为一个小的随机数，偏置可以初始化为0或小的正值。

    - 代价函数

        用于神经网络的代价函数通常在基本代价函数的基础上加一个正则项。用于线性模型的权重衰减办法也适用于深度神经网络，目前比较流行。又称为L2正则：

        $$J=J_0+\frac \lambda {2n}\sum _ww^2$$

        $J_0$代表原始的代价函数，后面就是L2正则化项，$\lambda$ 是正则项系数。$\frac 12$是计算方便，w的平方和求导会抵消掉。

        L2正则化有让w变小的效果，可以防止过拟合。小的权重表示该网络的复杂度更低，函数值在小区间内不会发生剧烈波动，对数据的拟合会更好。

        具体推导看第七章

        - 使用最大似然学习条件分布

            在神经网络中负对数似然和交叉熵等价，表示为：

            $$J(\theta)=-\mathbb E_{x,y~\hat p_{data}}\log p_{modle}(y|x)$$ 

            代价函数可以舍弃一些不依赖于模型的参数。

            使用负对数似然，明确一个模型p(y|x)后，直接对应损失函数为-log(p(y|x))。

            神经网络设计的重要点是，代价函数的梯度必需足够大和具有足够的预测性。饱和函数会破坏这个目标，因为它们会把梯度变得非常小。负对数似然在很多模型中都可以避免这个问题。

        - 学习条件使用量

            但机器学习不是为了得到一个完整概率分布，而是为了某个条件的统计值，可以使用变分法处理，具体参考 19.4 章

    - 输出单元

        任何可用作输出的神经网络单元，也可以被用作隐藏单元。

        定义 $h=f(x;\theta)$ 是已经提供的前馈网络。

        - 用于高斯输出分布的线性单元

            $$\hat y=W^Th+b$$

            这种称为线性单元，不具备非线性。这种输出层经常用于条件高斯分布的均值：

            $$p(y|x)=N(y;\hat y,I)$$

            因为线性的单元不会出现饱和，所以通常采用梯度下降来优化。

        - 用于伯努利输出分布的sigmoid单元

            具有二项分布的分类问题，通常使用 sigmoid 输出单元结合最大似然来实现。

            例如一张图片中有哪些动物，针对每一种动物类别就是一个伯努利分布，含有有这个分类和不含有这个分类。

            sigmoid 的输出单元定义：

            $$\hat y=\sigma(w^Th+b) $$

            >h表示与之衔接隐含层。$\sigma$ 就是sigmoid函数，$\sigma(x)=1/(1+exp(-x))$ 

            中间包含了两层含义,$z=w^Th+b$ 表示一个线性层计算，然后加上$\sigma(z)$的激活函数将z转化为概率。

            往下讨论一个z来定义y，得到y的概率分布，

            sigmoid激活函数不要求z是归一化，我们按结果往上推导：

            $$log(\hat p(y))=yz$$

            因为在似然函数加了log了，为了还原线性关系，所以第一层需要引入指数项。

            $$\hat p(y)=exp(yz)$$

            然后对接sigmoid激活函数：

            $$p(y)=\sigma(\hat p(y))$$

            $$p(y)=\frac {\exp(yz)}{1+\exp(yz)}$$

            由于$y \in \{0,1\}$ 导致 $yz$ 有偏

            所以我们需要将 y 的分布变为 $\{-1,1\}$

            所以调整 $yz \Rightarrow (2y-1)z$ 

            即：

            $$p(y)=\sigma((2y-1)z)$$

            从而得到损失函数为：

            $$J=-log(p(y))=-\log \sigma((2y-1)z)=\log\sigma((1-2y)z)$$

            这种在损失函数中引入指数的好处是，对数抵消了sigmoid中的指数函数，如果不抵消会造成由于sigmoid激活函数的饱和性导致后期的梯度消失。

            当 $(1-2y)z$ 为大的负数数时，sigmoid 函数趋近于0，导致log也趋近为0，出现饱和性，而大的负数出现的前提是 y=1 z为大的正数，y=0 z为大的负数，这样在学习中由于z的绝对值大，所以对于梯度下降来说，很容易更新到参数。  

        - 用于多项式输出分布的softmax单元

            sigmoid是二值型变量的分布，softmax是多项式分布。

            softmax 最常用于分类器，表示n个不同类别的概率分布。比较少见的是softmax可以用于某个模型变量的n个不同选项中选择。

            为了适应n个不同分类值的情况，创造一个向量 $\hat y$，其中每个元素是 $\hat y_i=P(y=i|x)$，$\hat y$需要在0~1之间，而且整个向量和需要等于1，同样我们定义到未归一化前面的对数概率

            $$z=W^Th+b$$

            其中 $z_i=log\hat P(y=i|x)$，往后，采用softmax对z进行归一化，softmax函数为：

            $$softmax(z)_i=\frac {exp(z_i)}{\sum _j exp(z_j)}$$

            取对数：

            $$\log softmax(z)_i=z_i-log\sum_j exp(z_j)$$

            表达式中 $z_i$ 是 前面线性单元的输出，所以不会饱和。当最大化似然时，第一项被拉高，第二项被拉低，所以 $log\sum_j exp(z_j)$ ，最终会倾向于 $max_jz_j$, 由于似然函数主要是惩罚最活跃的不正确项，所以当样本输出正确时，$z_i$ 和 $max_jz_j$ 最终几乎一样，导致正确的样本对代价函数的共享非常小，代价函数主要看不正确项。

            除了对数之外的目标函数对于softmax基本无效，因为如果不使用对数来抵消指数的增长，会在指数函数的输入为非常小的负数时，趋近与0，呈饱和性，导致梯度消失。

            softmax的问题在于当输入值之间的差异太大时，这些值会达到饱和，所以可以使用如下变体解决：

            $$softmax(z)=softmax(z-max_iz_i)$$

            这样就把整个z拉到0以下区间，起到对z的补偿作用。如果不这样做，会造成学习困难。

            softmax 会将 z 的和为1，这样实际上可以少一个参数，可以固定住一个参数，例如要求$z0=0$。不过这样做实际运用中效果都差不多，并且不减少参数的模型会更简单。

        - 其他的输出类型

            一般而言，如果定义了一个条件分布$p(y|x:\theta)$,最大似然原则建议我们采用 $-\log p(y|x;\theta)$ 作为代价函数。

            一般而言，我们认为神经网络的表示函数 $f(x;\theta)$ 的输出不是对y的直接预测，相反，$f(x;\theta)=w$ 提供了y分布的参数。我们的损失函数可以表示为 $-\log p(y;w(x))$

            例如，我们想要学习的是给定x时，y的条件高斯分布的方差。方差的似然函数只是观测y与他们的期望值的差值的平方平均。我们可能希望模型对不同的x值预测出y不同的方差，这种称异方差模型。在异方差下，我们简单的讲方差指定为f(x;\theta)的其中一个输出项，最常见的是使用一个对角精度矩阵：

            $$diag(\beta)$$

            这个适合梯度下降，支持对数，并且不涉及除法，除法在0附件会变得任意陡峭，会导致任意大的梯度，从而影响模型的稳定。

            如果用标准差来预测方差，涉及到除法和平方，平方运算也会导致在0附近梯度消失。

            学习一个协方差或者精度矩阵的困难度很高，一般不会这样做，涉及到特征值分解，运算量太大。

            如果要学习多峰回归，预测条件分布的值，该条件对于相同的x值在y的空间里有多个不同的峰值。这种情况下，可以用高斯混合输出，称为混合密度网络。高斯混合模型就是用高斯概率密度函数（正态分布曲线）精确地量化事物，它是一个将事物分解为若干的基于高斯概率密度函数（正态分布曲线）形成的模型。

            高斯混合，可以近似拟合任意形状的概率分布。并且能做到连续可微。具体应用例如：根据身高预测性别，模型分别做男女两个高斯分布或者人脸识别，分为成人小孩等。

            定义如下：

            $$p(y|x)=\sum _{i=1}^n p(c=i|x)N(y;u_i(x),\sum _i(x))$$

            神经网络需要有3个输出：

            1. 混合组件：$p(c=i|x))$，也就是多个高斯分布的比例，softmax为1

            2. 均值 $\mu_i(x)$，第i个高斯分布的均值，类似k聚类算法的中心

            3. 协方差 $\sum_i(x)$，第i个高斯分布对应的协方差矩阵

            中间涉及除法，可能导致数值不稳定，可以采用 梯度截断 或 启发式缩放梯度 来解决。

            [样例代码](./code/06-4.py)

1. 隐藏单元

    有些整流线性单元并不说有点都可微，例如relu(max{0,z})，在0点不可微，左导是0，右导是1，不过由于代价函数不会达到局部最小值，所以实际训练过程中表现良好。
            
    - 整流线性单元及扩展

        整流线性单元用 $g(z)=max{0,z}$

        通常用于：

        $$h=g(W^Tx+b)$$

        初始化时最好将b设置为一个小的正值例如0.1，这样一开始都处于激活状态，允许导数通过。

        扩展：当 z<0 时，采用一个非0的斜率 $a_i:h_i=g(z,a)_i=max(0,z_i)+a_imin(0,z_i)$

        - 绝对值整流，固定 $a_i=-1$，常用于图片中的对象识别，应用较广。

        - 渗漏整流线性单元，LeakyReLU，将$a_i$设置为一个类似0.01的小值。

        - PReLu，直接将$a_i$作为学习的参数。

        - maxout 单元将z分为每组具有k个值的组，每个maxout单元等于本组内的最大值。用于对x的多个方向的分段线性函数，有k个权重去参数化。maxout比其他整流需要更多的正则化。

        整流单元设计的原则，尽量接近线性，这样模型容易优化。

        - Swish $g(z)=x*sigmoid(\beta x)$ β是个常数或可训练的参数.Swish 具备无上界有下界、平滑、非单调的特性。Swish 在深层模型上的效果优于 ReLU。

        [演示代码](./code/06-2.py)

    - sigmoid 与 tanh

        sigmoid: $g(z)=a(z)$; tanh: $g(z)=2a(2z)-1$

        在引入整流线性单元之前，常用的激活函数是sigmoid或tanh。sigmoid在绝对值z很大时，两端都容易饱和，只有当z在0值附近才对输入敏感，所以不鼓励在将sigmoid用在前馈网络中的隐藏单元，但当代价函数有抵消sigmoid的饱和性时，可以作为输出单元。

        如果一定要用sigmoid激活函数，考虑 tanh 双正切函数，其表现比较好。因为tanh(0)=0,而sigmid(0)=0.5，而且tanh在0附近比sigmoid更类似单位函数，只要网络的激活能够保持的很小，就会工作很好。

        sigmoid函数在前馈网络以外用的多，有些模型不能使用分段线性激活函数时，会采用。

        [演示代码](./code/06-3.py)

    - 其他隐藏单元

        有其他的隐藏单元如$h=cos(Wx+b)$表现也不错。

        还有一种完全没有激活函数，可以考虑 $h=g(V^TU^Tx+b)$ ，这种方式提供了一种降低参数数量的办法。

        softmax通常用于输出，不过也是可以用于隐藏单元，可以作为开关，表示k个可能值的离散型随机变量的概率分布。

        还有其他的一些隐藏单元如下：

        - 径向基函数 $h_i=exp(-\frac 1{\sigma_i^2)}||W:,i-x||^2$ 只对x接近w时才活跃，大部分的x都饱和到0，很难优化。

        - softplus 函数 $g(a)=log(1+exp(a))$ ，这个是relu的平滑版本，但效果没有relu好，这个有点反直觉，但根据经验的确不如relu。

        - hard tanh 函数 $g(a)=max(-1,min(1,a))$ ，和tanh不同的是有界。

1. 架构设计

    神经网络一般采用链式结构。这种结构主要需要网络的深度和每一层的宽度

    - 万能近似性质和深度

        万能近似定理表明，一个前馈神经网络如果具有线性输出层和至少一层具有任何挤压性质的激活函数的隐藏层，只要给予足够多的隐藏单元，可以以任意精度去近似任何从一个有限维空间到另一个有限维空间的波莱尔可测函数，包括一切阶梯函数、一切连续函数和分段连续函数。

        前馈网络不存在万能的过程，既能验证训练集上的特殊样本，又能选择一个函数来扩展到训练集上没有点。

        虽然浅层神经网络可以达到我们希望的任意精度，但网络层可能大的没法实现；而网络的深度可以折叠输入空间，获得指数级的分段线性区域，降低了参数数量。

        另外选择深度模型，还有种情况，根据先验，认为模型应该涉及到某些函数的组合或相信学习的问题包含了一组潜在的变差因素。例如CNN浅层在参数超过2000万时就过拟合，而深层在参数超过6000万还表现良好。其表达了一种信念，函数应该由许多更简单的函数复合在一起得到，这样导致学习由更简单的表示所组成或者学习具有顺序依赖步骤的性质。

    - 其他架构上的考虑

        针对于特定的任务有很多特定的架构，例如处理计算机视觉的卷积神经网络和用于序列处理的循环神经网络。

        层不需要依次接入到链式结构中，可以架构一个主链，然后添加额外的架构特征，包括跳跃连接，这样使得梯度更容易由输出层流向更接近输入的层。

        架构设计的还有一个关键点是层与层的连接，默认是采用W的线性变换，这个每个输入都连接到每个输出，后面也可以将输入的每个单元连接到输出的一个子集，在特定的问题中使用，可以减少参数的数量。

1. 反向传播和其他的微分算法

    前向传播：使用前馈神经网络接收输入$x$并产生输出$\hat y$，信息向前流动。在训练过程中，前向传播可以持续向前直到产生一个标量代价函数$J(\theta)$。

    反向传播：允许来自代价函数的信息通过网络向后流动，以便计算梯度。

    反向传播的误解：

    > 反向传播仅用于计算梯度的方法，而另外一种算法如随机梯度下降，才用于指导学习。

    > 反向传播不仅适用于多层神经网络，原则上可以计算任意函数的导数。很多时候在计算其他其他函数的导数，而不仅仅局限于代价函数。

    1. 计算图

        用于更精确描述反向传播算法，使用计算图。除了节点表示变量，还有操作这个符号，可以组合在一起形成更复杂的函数。

    1. 微积分中的链式法则

        反向传播就是计算链式法则的方法。

        $$\nabla _xz=(\frac {\partial z}{\partial x})^T \nabla _yz$$

        这里的 $\frac {\partial z}{\partial x}$ 就是雅可比矩阵（函数的一阶偏导数排列成的矩阵），计算方式是由雅可比矩阵与梯度相乘得到。

        张量的链式法则为：

        $$\nabla _xz=\sum_j (\nabla _xY_j)\frac {\partial z}{\partial Y_j}$$

    1. 递归地使用链式法则来实现反向传播

        在反向传播时，如果遇到相同的子表达式，会利用梯度表来避免重复计算。

        $$\begin{aligned} 
        &grad\_table[u_n]=1  \\
        &for\ j = n-1\ down\ to\ 1\ do    \\
        &\qquad grad\_table[u_j]=\sum _{i:j\in Pa(u_i)}grad\_table[u_i]\frac {\partial u_i}{\partial u_j} \\
        &end\ for\\
        &return\ \{grad\_table[u_i]|i=1,\cdots,n_i\}
        \end{aligned}$$

    1. 全连接多层感知机中的反向传播计算

        MLP的计算逻辑：

        $l$: 网络的深度

        $W_i, i\in\{1,\cdots,l\}$： 模型的权重矩阵

        $b_i, i\in\{1,\cdots,l\}$： 模型的偏置参数

        $x$ 输入，$y$ 输出 

        $h_0=x$

        $for\ k=1,\cdots,l\ do$
        
        $\qquad a_k=b_k+W_kh_{k-l}$
        
        $\qquad h_k=f(a_k)$            #激活函数
        
        $end\ for$
        
        $\hat y=h_l$    #预测输出
        
        $J=L(\hat y,y)+\lambda\Omega(\theta)$   #损失函数 = 似然函数 + 正则项

        
    1. 符号到符号的导数

        在实际使用或训练时，采用一个数值来代替上面的x符号。

        在前向完成后，计算顶层的梯度：

        计算损失函数及似然函数的梯度

        $g\leftarrow\nabla_{\hat y}J=\nabla_{\hat y}L(\hat y,y)$

        $for\ k=l\ down\ to\ 1\ do$

        将层输出的梯度转换为非线性激活输入前的梯度，如果f是逐元素的，则逐元素相乘

        $\qquad g \leftarrow \nabla _{a_k} J=g * f'(a_k)$

        计算关于权重和偏置的梯度，包括正则项

        $\qquad \nabla _{b_k}J=g+\lambda \nabla _{b_k}\Omega(\theta)$

        $\qquad \nabla _{W_k}J=gh_{k-1}^T+\lambda \nabla _{W_k}\Omega(\theta)$

        上面公式里计算随后更低层的隐藏层传播梯度公式：

        $\qquad g \leftarrow \nabla _{h_{k-1}}=W_k^Tg$

        $end\ for$

        也可以用符号导图来描述反向传播算法。

    6. 一般化的反向传播

        符号图上每个节点对应一个张量 V，包含有标量、向量和矩阵。通常有下列子程序与 V 关联：

        - get_operation(V) : 返回用于计算V的操作

        - get_consumers(V) : 返回计算符号图中V的子节点的一组变量

        - get_inputs(V) : 返回计算符号图中v的父节点的一组变量

        get_operation操作也和求梯度 bprop 相关联。op.bprop 主要用于计算雅可比矩阵的向量积。例如，$C=AB$，C的梯度是G，如果调用brop来求A的梯度，则应该返回$GB^T$，如果求B的梯度，则返回$A^TG$。

        为了降低梯度计算，会使用动态规划的技巧，利用储存的中间导数对反向传播梯度表进行填充，这样避免重复计算公共表达式。

        $if\ V\ in\ grad\_table\ then$
       
        $\qquad return\ grad\_table[V]$

        $end\ if$

        $for\ C\ in\ get\_consumers(V)\ do$

        $\qquad op\leftarrow\ get\_operation(C)$

        $\qquad D\leftarrow\ build\_grad(C)$

        $\qquad G_i\leftarrow\ op.bprop(get\_inouts(C),V,D)$

        $end\ for$

        $G \leftarrow \sum_iG_i$

        $grad\_table[V]=G$

        $return\ G$

    - 实例：用于MLP训练的反向传播

        [演示代码](./code/06-5.py)
             













    



    




