深度前馈网络（多层感知机）是将输入的x映射到y，前馈网络定义了一个映射$y=f(x;\theta)$，并且学习参数$\theta$的值，使其得到最佳的函数近似。

这种是前向的，没有将输出的y重新加入到模型，也就是输出和模型之间没有反馈连接。区别于循环神经网络。

称为网络的原因是将很多不同的函数复合在一起，例如$y=f_3(f_2(f_1(x)))$，这种链式结构是神经网络中最常用的结构。链的全长称为模型的深度。前馈神经网络的最后一层称为输出层。中间层由于训练数据并没有给定每一层的输出，所以也称为隐藏层。隐藏层的维度决定了模型的宽度。可以将层想象为许多并行的单元组成，每个单元对应向量的节点称为神经元。神经元输入接收其他的神经元输出，并计算自己的激活值。

线性模型的好处是对凸函数能够高效可靠的拟合，但无法理解两个输入变量之间的相互作用。有一种技巧是对输入x做非线性处理后再用线性模型去拟合，例如使用核技巧。函数表示为 $y=f(x;\theta,w)=\phi(x;\theta)^Tw$，这样就有两个任务，一个是学习$\phi(x;\theta)$ 中的$\theta$；一个是$f(x';w)$中的$w$，其中$x'$是$\phi(x)$的映射。这样我们可以将已知的知识编码进网络来帮助泛化，所以只要找到一个非常广泛的函数族$\phi(x;\theta)$就可以改善线性模型的的泛化。

1. 实例：学习XOR






