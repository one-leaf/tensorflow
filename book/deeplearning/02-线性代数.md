- 标量、向量、矩阵、张量

    - 标量(scalar)： 单独的数

    - 向量(vector)： 一列数，可以看做空间中的点

    - 矩阵(matrix)： 二维数组

    - 张量(tensor)： 多维数组

    - 转置(transpose)，将矩阵按对角线为轴镜像: 

        $$ A_{i,j}^T=A_{j,i} $$

        >例子：
        >
        >$$ A = \begin{bmatrix} A_{1,1} & A_{1,2} \\ A_{2,1} & A_{2,2} \\ A_{3,1} & A_{3,2} \end{bmatrix}\Rightarrow A^T = \begin{bmatrix} A_{1,1} & A_{2,1} & A_{3,1} \\ A_{1,2} & A_{2,2} & A_{3,2} \end{bmatrix} $$

- 矩阵和向量相乘

    如果A的形状是(m,n), B的形状是(n,p), C的形状是(m,p)，则可以书写为 C = AB

    定义：

    $$ C_{i,j} = \sum_{k} A_{i,k}B_{k,j} $$

    注意不同于点积 A * B

    一些性质：
    
    $$ A(B+C) = AB + AC $$
    
    $$ A(BC) = (AB)C $$

    $$ x^Ty=y^Tx $$

    $$ (AB)^T=B^TA^T $$

    $$ x^Ty=(x^Ty)^T=y^Tx $$

    ```python
    import sympy as sp

    # 定义一个矩阵
    m = sp.Matrix([[1, 2, 3], [4, 5, 6]])

    # 获得转置
    n=m.T

    # m 转方阵
    m*n
    ```

    ```python
    import numpy as np

    # 定义一个矩阵
    m = np.array([[1, 2, 3], [4, 5, 6]])

    # 获得转置
    n=m.T

    # m 转方阵
    m@n 
    np.dot(m, n)
    np.matmul(m, n)
    ```

- 单位矩阵和逆矩阵

    任何向量与单位矩阵相乘都不会改变

    $$ \begin{bmatrix} 1&0&0 \\ 0&1&0 \\ 0&0&1 \end{bmatrix} = I_3 $$

    $I$ 就是单位矩阵

    $A$ 的逆矩阵为 $A^{-1}$ 

    $$ A^{-1}A=I_n $$

    ```python
    import sympy as sp

    # 获得一个单位矩阵 
    i=sp.eye(2)

    # 矩阵求逆
    m = sp.Matrix([[1, 3], [-2, 3]])
    m**(-1)

    # 检查矩阵和其逆相乘是否是单位矩阵
    i==m**(-1)*m
    ```

    ```python
    import numpy as np

    # 获得一个单位矩阵 
    i=np.eye(2)

    # 矩阵求逆
    m = np.array([[1, 3], [-2, 3]])
    np.linalg.inv(m)
    ```

- 线性相关和生成子空间

    线性方程组

    $$ Ax = b $$

    $A$ 为已知矩阵，$b$为已知向量，求解向量$x$

    展开：

    $$ A_{1,1}x_1+A_{1,2}x_2+...+A_{1,n}x_n=b_1 $$
    $$ A_{2,1}x_1+A_{2,2}x_2+...+A_{2,n}x_n=b_2 $$
    $$ ... $$
    $$ A_{m,1}x_1+A_{m,2}x_2+...+A_{m,n}x_n=b_m $$

    如果逆矩阵 $A^{-1}$ 存在，则上面有唯一解，反之则存在无解或存在无限多个解

    方程的解可以看做A的列向量从原点出发的不同方向，确定有多少种办法可以达到向量b；x的每个元素为我们应该沿着这个方向走的距离。

    $$ A_x=\sum_{i} x_iA_{:,i} $$

    这种操作就称为线性组合

    一组向量的生成子空间是原始向量线性组合后所能抵达的点的集合

    确定 $Ax=b$ 是否有解，相当于确定向量$b$是否在$A$列向量的生成子空间内。这个子空间称为$A$的列空间或$A$的值域。

    向量中的冗余称为线性相关，如果一组向量中的任意一个向量都不能表示成其他向量的线性组合，那么称为线性无关。

    列线性相关的矩阵被称为奇异的。如果矩阵A是一个方阵并且不是一个奇异的，可以用矩阵逆求解。

    ```python
    import sympy as sp

    # 求解ax=b
    a = sp.Matrix([[2,3,1],[4,2,3], [7,1,-1]])
    b = sp.Matrix([[4],[17],[1]])
    x = sp.symarray('x', (3,1))
    sp.solve(a*x-b) 
    ```

    ```python
    import numpy as np

    # 求解ax=b
    a = np.array([[2,3,1],[4,2,3], [7,1,-1]])
    b = np.array([[4],[17],[1]])
    np.linalg.solve(a, b) 
    ```

- 行列式

    行列式，$|A|$，det(A) ,是将方阵A映射到实数的函数，行列式等于矩阵特征值的乘积。一个矩阵的行列式就是一个平行多面体的体积，这个多面体的每条边对应着矩阵的列。

    2x2矩阵：

    $$A=\begin{bmatrix} x_{11} & x_{12} \\ x_{21} & x_{22} \end{bmatrix}$$
    $$|A|=x_{11}*x_{22}-x_{12}*x_{21}$$

    3x3 矩阵：

    $$A=\begin{bmatrix} a & b & c \\ d & e & f \\ g & h & i \end{bmatrix}$$
    $$|A|=a*\begin{bmatrix} e&f \\ h&i \end{bmatrix}-b*\begin{bmatrix} d&f \\ g&i \end{bmatrix}+c*\begin{bmatrix} d&e \\ g&h \end{bmatrix}$$
    $$|A|=a(ei-fh)-b(di-fg)+c(dh-eg)$$

    4x4 和更大的矩阵：
    
    $$A=\begin{bmatrix} a&b&c&d \\ e&f&g&h \\ i&j&k&l \\ m&n&o&p \end{bmatrix}$$

    $$
    |A|=a*\begin{bmatrix} &f&g&h \\ &j&k&l \\ &n&o&p \end{bmatrix}
    -b*\begin{bmatrix} e&&g&h \\ i&&k&l \\ m&&o&p \end{bmatrix}
    +c*\begin{bmatrix} e&f&&h \\ i&j&&l \\ m&n&&p \end{bmatrix}
    -d*\begin{bmatrix} e&f&g& \\ i&j&k& \\ m&n&o& \end{bmatrix}
    $$

    注意 +-+- 的规律，(+a... -b... +c... -d...) 依次类推。

    这个称为拉普拉斯展开。

    ```python
    import sympy as sp

    # 求行列式
    m = sp.Matrix([[1, 1, -1], [1, -2, 2], [-3, 1, 3]])
    m.det()
    ```    

    ```python
    import numpy as np

    # 求行列式
    m = np.array([[1, 1, -1], [1, -2, 2], [-3, 1, 3]])
    np.linalg.det(m)
    ```

- 范数

    范数是满足下列性质的函数:

    $$ f(x)=0 \Rightarrow x=0 $$
    $$ f(x+y) \le f(x) + f(y) $$
    $$ \forall \alpha\in \mathbb{R}, f(\alpha x)=|\alpha|f(x)  $$

    向量的范数：

    - 0 范数：向量中非零元素的个数
    - 1 范数：绝对值之和

    $$ ||x||_1=\sum _i|x_i| $$

    - 2 范数：通常意义上的模，各元素的平方和再开方

    $$ ||x||_2=\sqrt {\sum _ix_i^2} $$

    - 无穷范数或最大范数：就是取向量的最大值

    $$ ||x||_{\infty} =\max_j |x_i| $$
    $$ ||x||_{-\infty} =\min_j |x_i| $$

    衡量矩阵的大小为范数（norm）

    - p 范数：向量绝对值的p次方和的1/p次幂

    $$ ||x||_p = (\sum_i|x_i|^p)^{1/p} $$


    当p=2, p 范数等于L2范数，称为欧几里得范数，表示从原点出发到向量x确定的点的欧几里得距离

    L2范数省略开平方后称为平方L2范数。

    平方L2范数在原点附近的增长很缓慢，所有某些场合，采用L1范数，简化为：

    $$ ||x||_1=\sum_i|x_i| $$

    L1范数有时候可以作为统计非0元素数目的替代。

    矩阵的范数：

    - 1 范数，列和范数，所有矩阵列向量之和的最大值

    $$ ||A||_1 = max_j \sum_{i=1}^m|a_{i,j}| $$

    - 2 范数，谱范数，即$A^TA$矩阵的最大特征值的开平方。

    $$ ||A||_2 = \sqrt {\lambda_1} $$

    $\lambda_1$为$A^TA$的最大特征值。

    - 无穷范数或最大范数，行和范数，即所有矩阵行向量绝对值之和的最大值。

    $$ ||A||_F = \max_i\sum_{j=1}^n|a_{i,j}|$$

    - 范数也可以用来衡量矩阵的大小，称为Frobenius范数，简称F范数，就是矩阵的每个元素的平方和的开方。

    $$ ||A||_F = \sqrt {\sum_{i,j}A_{i,j}^{2}} $$

    两个向量的点积可以用范数来表示:

    $$ x^Ty=||x||_2||y||_2cos\theta $$

    $\theta$ 表示 x 和 y 之间的夹角 

    >例子，求 矩阵$A =\begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}$ 的L2范数
    >
    >矩阵范数定义
    >
    >$$ ||A||^2_2= \sqrt {\lambda_1} $$
    >
    >$\lambda_1$为矩阵$A$的最大特征值
    >
    >$$ \lambda = A^T A=\begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}*\begin{bmatrix} 1 & 4 \\2 & 5 \\ 3 & 6 \end{bmatrix}$$
    >$$=\begin{bmatrix} 1*1+2*2+3*3 & 1*4+2*5+3*6 \\ 4*1+5*2+6*3 & 4*4+5*5+6*6 \end{bmatrix}$$
    >$$=\begin{bmatrix} 14 & 32 \\ 32 & 77 \end{bmatrix}$$
    >$$|\lambda E-A|=\begin{bmatrix} \lambda -14 & -32 \\ -32 & \lambda -77 \end{bmatrix}=0$$
    >$$\Rightarrow(\lambda-14)*(\lambda-77)-(-32*-32)=0$$
    >$$\Rightarrow\lambda^2-91\lambda+54=0$$
    >$$\Rightarrow{(\lambda-{\frac {91}{2}})} ^2-\frac {91^2}{4}+54=0$$
    >$$\lambda= \pm\sqrt{\frac {91^2}{4}-54}+\frac{91}{2}$$
    >$$\lambda=[90.4\ 0.6]$$
    >$$||A||^2_2=\sqrt{\max_i\lambda}=\sqrt {90.4}=9.5079$$

    ```python
    import sympy as sp

    m = sp.Matrix([[1, 2, 3], [4, 5, 6]])
    # 求L2范数
    m.norm(2)
    ```

    ```python
    import numpy as np

    m = np.array([[1, 2, 3], [4, 5, 6]])
    # 求L2范数
    np.linalg.norm(m, 2)
    ```


- 特殊类型的矩阵和向量

    对角矩阵：只有主对角线上含有非零元素，其余都是零。

    对称矩阵：矩阵转置后和原矩阵相等。 $A=A^T$

    单位向量：具有单位范数的向量 $||x||_2=1$

    正交矩阵：行向量和列向量是分别标准正交的方阵， $A^TA=AA^T=I$

- 特征分解

    将矩阵分解为一组特征向量和特征值。

    方阵A的特征向量是指与A相乘后相当于对该向量进行缩放的非零向量v

    $$ Av=\lambda v $$

    $\lambda$ 就是这个特征向量对应的特征值

    >例子：求$A=\begin{bmatrix} 1& 1 & -1\\1 &-2&2 \\-3&1&3 \end {bmatrix}$ 的特征值
    >
    >1. 根据特征多项式得
    >
    >    $$|\lambda E-A|=\begin{bmatrix} \lambda-1& -1 & 1\\-1 &\lambda+2&-2 \\3&-1&\lambda-3 \end {bmatrix}=0$$
    >
    >2. 第1行减去第三行，得到一个0
    >
    >    $$=\begin{bmatrix} \lambda-4& 0 & 4-\lambda\\-1 &\lambda+2&-2 \\3&-1&\lambda-3 \end {bmatrix}$$
    >
    >3. 第3列加上第一列，得到第二个0
    >
    >    $$=\begin{bmatrix} \lambda-4& 0 & 0\\-1 &\lambda+2&-3 \\3&-1&\lambda \end {bmatrix}$$
    >
    >4. 展开多项式
    >
    >    $$=(\lambda-4)\begin{bmatrix} \lambda+2&-3 \\-1&\lambda \end {bmatrix} - 0*... + 0*...$$
    >    $$=(\lambda-4)((\lambda+2)(\lambda)-(-1*-3))$$
    >    $$=(\lambda-4)(\lambda^2+2\lambda-3)$$
    >    $$=(\lambda-4)(\lambda-1)(\lambda+3)=0$$
    >
    >5. 解得
    >
    >    $$\lambda=[4,1,-3]$$
    >
    >6. 带入方程，求解再归一化，即特征向量为：
    >
    >   $$\begin{bmatrix}v(4)&v(1)&v(-3)\end {bmatrix}=\begin{bmatrix} -0.22& 0.27 & 0.94\\-0.58 &-0.58&-0.58 \\-0.30&0.90&-0.30 \end {bmatrix}$$

    ```python
    import sympy as sp

    m = sp.Matrix([[1, 1, -1], [1, -2, 2], [-3, 1, 3]])
    # 特征值
    m.eigenvals()

    # 特征向量，注意这里没有进行归一化，如果要归一化调用
    # .normalized().evalf()
    # 例如：
    # m.eigenvects()[0][2][0].normalized().evalf()
    m.eigenvects()
    ```

    ```python
    import numpy as np

    # 特征值 和 特征向量
    m = np.array([[1, 1, -1], [1, -2, 2], [-3, 1, 3]])
    np.linalg.eig(m)
    ```

    矩阵A有n个线性无关的特征向量 {$v^{(1)},...,v^{(1)}$,对应的特征值{$\lambda^{(1)},...,\lambda^{(n)}$}。将特征向量连接成一个矩阵，每一列是一个特征向量: $V=[v^{(1)},...,v^{(n)}]$，类似特征值也可以接连成一个向量，因此A的特征分解可以记为：

    $$A=Vdiag(\lambda)V^{(-1)}$$

- 奇异值分解

    将矩阵分解为奇异向量和奇异值。非方阵只能用奇异值分解。

    $$A=UDV^T$$

    假设 A:(m,n) 则 U:(m,m) D:(m,n) V:(n,n), 其中U和V都为正交矩阵，D为对角矩阵（不一定是方阵）

    对角矩阵D的对角线上的元素为 A 的奇异值，U的列向量为左奇异向量，V的列向量为右奇异向量。

    ```python
    from mpmath import mp

    # svd 奇异值分解
    m = mp.matrix([[1, 1, -1], [1, -2, 2], [-3, 1, 3]])
    mp.svd_r(m)
    ```

    ```python
    import numpy as np

    # svd 奇异值分解
    m = np.array([[1, 1, -1], [1, -2, 2], [-3, 1, 3]])
    np.linalg.svd(m)
    ```

- 伪逆

    解非方矩阵和奇异矩阵用，是满足几何约束的条件下的最优解。

    $$ A^+ = VD^+U^T $$ 

    U、D、V是A奇异值分解后得到的矩阵

    ```python
    import sympy as sp

    # 求伪逆
    m = sp.Matrix([[1, 1, -1], [1, -2, 2], [-3, 1, 3]])
    m.inv()
    ```

    ```python
    import numpy as np

    # 求伪逆
    m = np.array([[1, 1, -1], [1, -2, 2], [-3, 1, 3]])
    np.linalg.inv(m)
    ```

- 迹运算

    迹运算返回的是矩阵对角元素的和：

    $$ Tr(A) = \sum _iA_{i,i} $$

    相当于另外一种范式

- 主成分分析（PCA）

    简单的机器学习算法，通过压缩信息，编码和解码

- 协方差矩阵

    - 基本概念
    
        - 均值

            $$\overline s=\frac 1n\sum _{i=1}^nX_i$$

        - 标准差

            $$\sigma=\sqrt {\frac 1n\sum _{i=1}^n(x_i-\overline x)^2}$$

        - 方差

            $$\sigma^2=\frac 1n\sum _{i=1}^n(x_i-\overline x)^2$$
        
        无偏估计时，将n改为n-1
    
    - 协方差

        需要度量两个相同长度向量之间的关系，仿造方差的定义定义协方差矩阵：

        $$cov(x,y)=\frac 1n\sum _{i=1}^n(x_i-\overline x)(y_i-\overline y)$$

        同样无偏估计时，将n改为n-1

        协方差值相关系数定义为：
        
        $$\eta=\frac {cov(x,y)}{\sigma_x\sigma_y}$$
        
        协方差值相关系数在[-1,1]之间。0是无关，1是正相关，-1是负相关。


    - 协方差矩阵

        用于度量各个维度之间的相关性，定义：

        $$X_{m,n}=\begin{bmatrix} x_{11}&x_{12}&\cdots&x_{1n}
        \\ x_{21}&x_{22}&\cdots&x_{2n} 
        \\ \vdots&\vdots&\vdots&\vdots 
        \\ x_{m1}&x_{m2}&\cdots&x_{mn} 
        \end {bmatrix}=\begin{bmatrix} C_1&C_2&\cdots&C_n \end {bmatrix}$$

        协方差矩阵为：

        $$cov(X)=\frac 1{m-1}\begin{bmatrix} cov(c_1,c_1)&cov(c_1,c_2)&\cdots&cov(c_1,c_n)
        \\ cov(c_2,c_1)&cov(c_2,c_2)&\cdots&cov(c_2,c_n) 
        \\ \vdots&\vdots&\vdots&\vdots 
        \\ cov(c_n,c_1)&cov(c_n,c_2)&\cdots&cov(c_n,c_n)
        \end {bmatrix}$$

        协方差cov(i,j)=（第i列的所有元素-第i列的均值）*（第j列的所有元素-第j列的均值）

        可见，协方差矩阵是一个对称矩阵，对角线是各个维度的方差。

        对于矩阵的协方差矩阵计算如下：

        $$cov(A)=\frac 1{n-1}A^TA$$

        >例子：
        >
        >求 $A=\begin{bmatrix} 1&2&3 \\ 3&1&1 \end {bmatrix}$ 的协方差矩阵
        >
        >$$\begin{bmatrix} 1&2&3 \\ 3&1&1 \end {bmatrix}=\begin{bmatrix} C_1&C_2&C_3 \end {bmatrix}$$
        >$$\overline C=\begin{bmatrix} \overline {C_1}&\overline {C_2}&\overline {C_3} \end {bmatrix}=\begin{bmatrix} 2&1.5&2 \end {bmatrix}$$
        >$$X=A-\overline C=\begin{bmatrix} -1&0.5&1 \\ 1&-0.5&-1 \end {bmatrix}$$
        >$$cov(A)=\frac 1{n-1}X^TX=\frac 1{2-1}\begin{bmatrix} 2&-1&-2 \\ -1&0.5&1 \\-2&1&2 \end {bmatrix}$$

        ```python
        import numpy as np

        # 求协方差矩阵
        m = np.array([[1, 2, 3], [3, 1, 1]])
        np.cov(m.T)
        ```

- 实用举例

    mnist 手写体数据识别：

    输入x为[28,28]手写体图片

    $$x=\begin{bmatrix} x_{(1,1)}&...&x_{(1,28)}\\ \vdots &\vdots &\vdots \\ x_{(28,1)}&...&x_{(28,28)} \end{bmatrix} $$

    扁平化 $x$ 得 $Shape(X) = [1,28*28] = [1, 784]$ 的矩阵

    $$X=\begin{bmatrix} x_{(1)} \cdots x_{(784)} \end{bmatrix} $$

    定义参数 w 为 [28*28, 10] = [784, 10] 的矩阵， 10为手写体数字[0~9]，共10个分类。

    $$w=\begin{bmatrix} w_{(1,1)}&...&w_{(1,10)}\\ \vdots &\vdots &\vdots \\ w_{(784,1)}&...&w_{(10,784)} \end{bmatrix}$$

    定义偏置 b 为 [1, 10] = [1, 10] 的矩阵

    $$b=\begin{bmatrix} b_{(1)} \cdots b_{(10)}\end{bmatrix}$$

    根据线性方程组得输出y，手写体数字[0~9]的每一个数字的概率：

    $$y=X*w+b$$
    $$=\begin{bmatrix} x_{(1)} \cdots x_{(784)} \end{bmatrix}  * \begin{bmatrix} w_{(1,1)}&...&w_{(1,10)}\\ \vdots &\vdots &\vdots \\ w_{(784,1)}&...&w_{(10,784)} \end{bmatrix} + \begin{bmatrix} b_{(1)} \cdots b_{(10)}\end{bmatrix}$$
    $$=\begin{bmatrix} y_{(1)} \cdots y_{(10)}\end{bmatrix}$$

    然后加上 $y_{pred} = softmax(y)$，就构成了一个最基本的神经网络。


