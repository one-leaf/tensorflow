本节将如何使用深度学习来解决不同商业领域的应用。

1. 大规模深度学习

    深度学习的基本思想基于联结主义，一个最关键的因素就是网络规模的巨大提升。

    1. 快速的CPU实现

        现在可以多CPU并行计算，或者针对CPU的某些特性加强。

    1. GPU实现

        NVIDIA的CUDA编程语言提供了神经网络的理想平台，但和CPU很不相同。

    1. 大规模的分布式实现

        数据并行是很容易实现，模型并行也是可行的，每个机器负责模型的一个部分。

        但梯度下降会出现麻烦，不过可以用异步随机梯度下降来解决，即利用锁来处理；新的提出了多机器无锁的梯度下降法，是采用参数服务器管理而非存储到共用的内存中。

    1. 模型压缩

        由于需要部署到移动端或资源有限的环境，减少推断开销的一个关键策略就是模型压缩。基本思想是用一个更小的模型代替原始耗时的模型。

        主要是样本数不够的原因，所以才使用巨大的参数量超过任务的需求。可以先训练一个大的模型，然后通过这个大模型源源不断的参数样本，来训练一个新的更小的模型。

        或者直接在原始训练上训练一个更小的模型，但只是为了复制模型的其它特征。（老师-学生训练）

    1. 动态结构

        加速数据处理的办法是构造一个系统，用动态结构，对于输入的数据，数据处理系统可以动态的决定运行神经网络系统的哪一部分。也称为条件计算。

        在分类器中加速推断中可以使用级联的分类器，先分类为低容量高召回率的，最后一个训练为高精度的。这样可以依次推断，中间任何一层拒绝，就选择抛弃，这样就不需要为每个样本付出完全推断的成本。例如Google的街景地址编号识别，第一步先查找地址编号，第二步再进行转录。

        还可以用类似的选通器，将当前给定的输入使用几个专家网络来计算输出。每个选通器为每个专家输出一个概率或权重，并且最终输出由个个专家输出的加权组合获得。但如果每个选通器只选择单个专家网络，就可以加速推断和训练。

        还有一种动态结构是开关，隐藏单元可以根据情况从不同的单元接收输入。这种动态路由的方法可以理解为注意力机制。但这种机制没有得到大规模应用的验证有效性。

    1. 深度网络的专用硬件实现

        目前有 ASIC 或 FPGA 的实现。

1. 计算机视觉

    包括图像识别，图像标注、图像合成等。

    1. 预处理

        - 有些尺寸固定的模型需要裁剪或缩放调整图像尺寸，有些卷积可接受可变大小的输入，并动态调整池化区域，以保持输出大小恒定。

        - 数据增强通过不同位置的裁剪或其它方式可以将同一输入的不同版本传给模型，有助于减少泛化误差。

        - 对于大型数据集和大规模模型训练时，预处理通常不重要，例如AlexNet系统只有一个预处理，对每一个像素减去训练样本的平均值。

        1. 对比度归一化

            有张图片的张量为 $X \in \mathbb R^{h,w,3}$ 

            $X_{i,j,1}$ 表示第i行第j列红色的强度，$X_{i,j,2}$ 表示第i行第j列绿色的强度，$X_{i,j,1}$ 表示第i行第j列蓝色的强度。则整个图片的对比度为：

            $$\sqrt {\frac {1}{3hw}\sum_{i=1}^h\sum_{j=1}^w\sum_{k=1}^3(X_{i,j,k}-\bar X)^2}$$

            $\bar X$ 是整个图片的平均强度，为：

            $$\bar X={\frac {1}{3hw}\sum_{i=1}^h\sum_{j=1}^w\sum_{k=1}^3X_{i,j,k}}$$

            全局对比度归一化（GCN）定义为：

            $$X'_{i,j,k}=s\frac {X_{i,j,k}-\bar X}{max[\epsilon,\sqrt{\lambda+{\frac {1}{3hw}\sum_{i=1}^h\sum_{j=1}^w\sum_{k=1}^3(X_{i,j,k}-\bar X)^2}}]}$$

            除了全局对比度归一化，还有白化（sphering 或 whitening），将图像进行PCA分解。

            全局对比度归一化不能突出想要突出的图像特征，如边缘和角，所以产生了局部对比归一化，将对比度在每一个小窗上进行，而不是作为一个整体进行。

            [演示代码](./code/12-1.py)

        1. 数据集增强

            对图像进行一些变化，如随机翻转或旋转等。还有加上颜色的随机扰动以及非线性几何变形。

            [演示代码](./code/12-2.py)

1. 语音识别

    语音识别是将一段包含了自然语言发音的声学信号投影到对应说话人的词序列上。

    自动语音识别也叫ASR任务，即构造一个函数 $f_{ASR}^*$ 能够在给定声学序列X的情况下计算最有可能的语言序列y：

    $$f_{ASR}^*(X)=arg\max_y P^*(y|X=X)$$

    其中 $P^*$ 是给定输入值X时对应目标y的真实条件分布。

    早期，最先进的语音识别模型是隐马尔可夫模型和高斯混合模型的结合。GMM对声学特征和音素建模，HMM对音素序列建模。

    然后发展出用无监督学习称作受限玻尔兹曼机的无向概率模型，对输入建模，用来构造一个深度前馈网络。将错误率从26%降到20%。

    然后引入了各种初始化，整流线性单元和Dropout技术，发现无监督的预训练是没有必要的。

    现在的一个创新点是卷积网络的应用，将输入的频谱不是当做一个长的向量，而是当做一个图像，一个轴对应时间，一个轴对应谱分量的频率。

    另外的突破是深度的长短期记忆循环网络的应用，使用了帧-音素排列的MAP推断，用在LeCun以及CTC框架。这个将错误率降低到17.7%。

    另一个端到端的深度学习语音识别的最新方法是，让系统学习如何利用语音层级的信息排列声学层级的信息。

1. 自然语言处理

    自然语言处理(NLP)是让计算机可以使用人类语言。

    1. n-gram

        语言模型定义了自然语言中标记序列的概率分布。根据模型的设计，标记可以是词、字符甚至字节。标记总是离散的实体。最早的成功语言模型是基于固定长度序列的标记模型，称为n-gram，一个n-gram是一个包含n个标记的序列。

        基于n-gram的模型定义一个条件概率，给定前n-1个标记后的第n个标记的条件概率。该模型使用这些条件分布的乘积定义较长序列的概率分布：

        $$P(x_1,...,x_{\tau})=P(x_1,..,x_{n-1})\prod_{t=n}^{\tau}P(x_t|x_{t-n+1,...,x_{t-1}})$$

        当n=1，模型称为一元语法，n=2，称为二元语法，n=3，称为三元语法。

        通常同时训练 n-gram 和 n-1 gram 模型，可以通过查找两个存储的概率来计算：

        $$P(x_t|x_{t-n+1},...,x_{t-1})=\frac {P_n(x_t|x_{t-n+1},...,x_{t})}{P_{n-1}(x_t|x_{t-n+1},...,x_{t-1})}$$

        模型的限制是 $P_n$ 可能为0或$P_{n-1}$为0，这样导致无法计算梯度。所以为了避免出现这种情况，大多数的n-gram的模型都采用某种形式的平滑。一种办法是将所有可能的下一个都添加一个小值，另外一个办法是包含高阶和低阶n-gram的模型的混合模型，其中由高阶模型提供更多的容量，低阶尽可能避免零计数。如果上下文的概率太小不能使用高阶模型，就回退查找低阶的n-gram。

        经典的n-gram模型容易引起维数灾难。

        为了提高n-gram模型的统计效率，基于类的语言模型被引入，同一类别的词共享词之间的统计强度。

    1. 神经语言模型

        神经语言模型（NLM）是克服维度灾难的语言模型，使用词的分布式表示对自然语言序列建模。模型可以识别两个相似的词，并且不丧失将每个词编码为彼此不同的能力。

        将这些词称为词嵌入（Word Embedding），将原始符号视为维度等于词表大小的空间的点，词表示将这些点嵌入到较低维的特征空间中。在原始空间中，每个词由一个one-hot的向量表示，因此每个词的距离相等；在嵌入空间中，词彼此相接近。

        当然其他领域的神经网络也可以定义嵌入，如卷积网络的隐藏层提供图像嵌入。因为自然语言不在实值向量空间上，所以NLP的从业者对嵌入的想法更感兴趣。

    1. 高维输出

        在自然语言应用中，模型通常输出词，而不是字母。由于词汇表通常很大，在词的选择上表示输出分布的计算成本可能非常高。包括全连接层和Softmax函数，都需要将整个词汇表做归一化，导致最终在计算交叉熵时会出现很多困难。

        1. 使用短列表

            将词汇表分为最常见词汇的短列表和较稀有词汇的尾列表，为了组合预测，神经网络还需要预测在上下文C后面出现的词位于尾列表的概率。

            可以通过添加sigmoid输出单元估计来实现这个预测。

            缺点是神经网络的潜在泛化优势仅限于最常见的词，所以导致最没用。

        1. 分层softmax

            这种层次结构是先建立词的类别，然后是词类别的类别，然后是词类别的类别的类别等。最后的叶子是词语。概率则涉及在该词的所有路径上求和。

            一般基于深度为2的树，可以得到大部分的计算益处。就是每个节点分为2个子节点。

            缺点是，在实践过程中，总是倾向于更差的测试结果，可能是词的类别选择的不好。

        1. 重要采样

            加速神经网络语言模型训练的一种方式是避免明确计算所有未出现在下一位置的词对梯度的贡献。枚举所有词的计算成本太高，所以可以仅采样词的子集。