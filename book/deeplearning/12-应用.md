本节将如何使用深度学习来解决不同商业领域的应用。

1. 大规模深度学习

    深度学习的基本思想基于联结主义，一个最关键的因素就是网络规模的巨大提升。

    1. 快速的CPU实现

        现在可以多CPU并行计算，或者针对CPU的某些特性加强。

    1. GPU实现

        NVIDIA的CUDA编程语言提供了神经网络的理想平台，但和CPU很不相同。

    1. 大规模的分布式实现

        数据并行是很容易实现，模型并行也是可行的，每个机器负责模型的一个部分。

        但梯度下降会出现麻烦，不过可以用异步随机梯度下降来解决，即利用锁来处理；新的提出了多机器无锁的梯度下降法，是采用参数服务器管理而非存储到共用的内存中。

    1. 模型压缩

        由于需要部署到移动端或资源有限的环境，减少推断开销的一个关键策略就是模型压缩。基本思想是用一个更小的模型代替原始耗时的模型。

        主要是样本数不够的原因，所以才使用巨大的参数量超过任务的需求。可以先训练一个大的模型，然后通过这个大模型源源不断的参数样本，来训练一个新的更小的模型。

        或者直接在原始训练上训练一个更小的模型，但只是为了复制模型的其它特征。（老师-学生训练）

    1. 动态结构

        加速数据处理的办法是构造一个系统，用动态结构，对于输入的数据，数据处理系统可以动态的决定运行神经网络系统的哪一部分。也称为条件计算。

        在分类器中加速推断中可以使用级联的分类器，先分类为低容量高召回率的，最后一个训练为高精度的。这样可以依次推断，中间任何一层拒绝，就选择抛弃，这样就不需要为每个样本付出完全推断的成本。例如Google的街景地址编号识别，第一步先查找地址编号，第二步再进行转录。

        还可以用类似的选通器，将当前给定的输入使用几个专家网络来计算输出。每个选通器为每个专家输出一个概率或权重，并且最终输出由个个专家输出的加权组合获得。但如果每个选通器只选择单个专家网络，就可以加速推断和训练。

        还有一种动态结构是开关，隐藏单元可以根据情况从不同的单元接收输入。这种动态路由的方法可以理解为注意力机制。但这种机制没有得到大规模应用的验证有效性。

    1. 深度网络的专用硬件实现

        目前有 ASIC 或 FPGA 的实现。

1. 计算机视觉

    包括图像识别，图像标注、图像合成等。

    1. 预处理

        - 有些尺寸固定的模型需要裁剪或缩放调整图像尺寸，有些卷积可接受可变大小的输入，并动态调整池化区域，以保持输出大小恒定。

        - 数据增强通过不同位置的裁剪或其它方式可以将同一输入的不同版本传给模型，有助于减少泛化误差。

        - 对于大型数据集和大规模模型训练时，预处理通常不重要，例如AlexNet系统只有一个预处理，对每一个像素减去训练样本的平均值。

        1. 对比度归一化

            有张图片的张量为 $X \in \mathbb R^{h,w,3}$ 

            $X_{i,j,1}$ 表示第i行第j列红色的强度，$X_{i,j,2}$ 表示第i行第j列绿色的强度，$X_{i,j,1}$ 表示第i行第j列蓝色的强度。则整个图片的对比度为：

            $$\sqrt {\frac {1}{3hw}\sum_{i=1}^h\sum_{j=1}^w\sum_{k=1}^3(X_{i,j,k}-\bar X)^2}$$

            $\bar X$ 是整个图片的平均强度，为：

            $$\bar X={\frac {1}{3hw}\sum_{i=1}^h\sum_{j=1}^w\sum_{k=1}^3X_{i,j,k}}$$

            全局对比度归一化（GCN）定义为：

            $$X'_{i,j,k}=s\frac {X_{i,j,k}-\bar X}{max[\epsilon,\sqrt{\lambda+{\frac {1}{3hw}\sum_{i=1}^h\sum_{j=1}^w\sum_{k=1}^3(X_{i,j,k}-\bar X)^2}}]}$$

            除了全局对比度归一化，还有白化（sphering 或 whitening），将图像进行PCA分解。

            全局对比度归一化不能突出想要突出的图像特征，如边缘和角，所以产生了局部对比归一化，将对比度在每一个小窗上进行，而不是作为一个整体进行。

            [演示代码](./code/12-1.py)

        1. 数据集增强

            对图像进行一些变化，如随机翻转或旋转等。还有加上颜色的随机扰动以及非线性几何变形。

            [演示代码](./code/12-2.py)

1. 语音识别

    语音识别是将一段包含了自然语言发音的声学信号投影到对应说话人的词序列上。

    自动语音识别也叫ASR任务，即构造一个函数 $f_{ASR}^*$ 能够在给定声学序列X的情况下计算最有可能的语言序列y：

    $$f_{ASR}^*(X)=arg\max_y P^*(y|X=X)$$

    其中 $P^*$ 是给定输入值X时对应目标y的真实条件分布。

    早期，最先进的语音识别模型是隐马尔可夫模型和高斯混合模型的结合。GMM对声学特征和音素建模，HMM对音素序列建模。

    然后发展出用无监督学习称作受限玻尔兹曼机的无向概率模型，对输入建模，用来构造一个深度前馈网络。将错误率从26%降到20%。

    然后引入了各种初始化，整流线性单元和Dropout技术，发现无监督的预训练是没有必要的。

    现在的一个创新点是卷积网络的应用，将输入的频谱不是当做一个长的向量，而是当做一个图像，一个轴对应时间，一个轴对应谱分量的频率。

    另外的突破是深度的长短期记忆循环网络的应用，使用了帧-音素排列的MAP推断，用在LeCun以及CTC框架。这个将错误率降低到17.7%。

    另一个端到端的深度学习语音识别的最新方法是，让系统学习如何利用语音层级的信息排列声学层级的信息。

1. 自然语言处理

    自然语言处理(NLP)是让计算机可以使用人类语言。

    1. n-gram

        语言模型定义了自然语言中标记序列的概率分布。根据模型的设计，标记可以是词、字符甚至字节。标记总是离散的实体。最早的成功语言模型是基于固定长度序列的标记模型，称为n-gram，一个n-gram是一个包含n个标记的序列。

        基于n-gram的模型定义一个条件概率，给定前n-1个标记后的第n个标记的条件概率。该模型使用这些条件分布的乘积定义较长序列的概率分布：

        $$P(x_1,...,x_{\tau})=P(x_1,..,x_{n-1})\prod_{t=n}^{\tau}P(x_t|x_{t-n+1,...,x_{t-1}})$$

        当n=1，模型称为一元语法，n=2，称为二元语法，n=3，称为三元语法。

        通常同时训练 n-gram 和 n-1 gram 模型，可以通过查找两个存储的概率来计算：

        $$P(x_t|x_{t-n+1},...,x_{t-1})=\frac {P_n(x_t|x_{t-n+1},...,x_{t})}{P_{n-1}(x_t|x_{t-n+1},...,x_{t-1})}$$

        模型的限制是 $P_n$ 可能为0或$P_{n-1}$为0，这样导致无法计算梯度。所以为了避免出现这种情况，大多数的n-gram的模型都采用某种形式的平滑。一种办法是将所有可能的下一个都添加一个小值，另外一个办法是包含高阶和低阶n-gram的模型的混合模型，其中由高阶模型提供更多的容量，低阶尽可能避免零计数。如果上下文的概率太小不能使用高阶模型，就回退查找低阶的n-gram。

        经典的n-gram模型容易引起维数灾难。

        为了提高n-gram模型的统计效率，基于类的语言模型被引入，同一类别的词共享词之间的统计强度。

        [演示代码](./code/12-4.py)

    1. 神经语言模型

        神经语言模型（NLM）是克服维度灾难的语言模型，使用词的分布式表示对自然语言序列建模。模型可以识别两个相似的词，并且不丧失将每个词编码为彼此不同的能力。

        将这些词称为词嵌入（Word Embedding），将原始符号视为维度等于词表大小的空间的点，词表示将这些点嵌入到较低维的特征空间中。在原始空间中，每个词由一个one-hot的向量表示，因此每个词的距离相等；在嵌入空间中，词彼此相接近。

        当然其他领域的神经网络也可以定义嵌入，如卷积网络的隐藏层提供图像嵌入。因为自然语言不在实值向量空间上，所以NLP的从业者对嵌入的想法更感兴趣。

        [演示代码](./code/12-5.py)

    1. 高维输出

        在自然语言应用中，模型通常输出词，而不是字母。由于词汇表通常很大，在词的选择上表示输出分布的计算成本可能非常高。包括全连接层和Softmax函数，都需要将整个词汇表做归一化，导致最终在计算交叉熵时会出现很多困难。

        1. 使用短列表

            将词汇表分为最常见词汇的短列表和较稀有词汇的尾列表，为了组合预测，神经网络还需要预测在上下文C后面出现的词位于尾列表的概率。

            可以通过添加sigmoid输出单元估计来实现这个预测。

            缺点是神经网络的潜在泛化优势仅限于最常见的词，所以导致最没用。

        1. 分层softmax

            这种层次结构是先建立词的类别，然后是词类别的类别，然后是词类别的类别的类别等。最后的叶子是词语。概率则涉及在该词的所有路径上求和。

            一般基于深度为2的树，可以得到大部分的计算益处。就是每个节点分为2个子节点。

            缺点是，在实践过程中，总是倾向于更差的测试结果，可能是词的类别选择的不好。

        1. 重要采样

            加速神经网络语言模型训练的一种方式是避免明确计算所有未出现在下一位置的词对梯度的贡献。枚举所有词的计算成本太高，所以可以仅采样词的子集。

            梯度可以写成如下形式：

            $$\frac {\partial log P(y|C)}{\partial \theta}=\frac {\partial log softmax_y(a)}{\partial \theta}$$

            $$=\frac {\partial}{\partial \theta}log\frac {e^{a_y}}{\sum_ie^{a_i}}$$

            $$=\frac {\partial}{\partial \theta}(a_y-log\sum_i e^{a_i})$$

            $$=\frac {\partial a_y}{\partial \theta}-\sum_iP(y=i|C)\frac {\partial a_i}{\partial \theta}$$

            其中 a 是 softmax 激活向量，每个词对应一个元素。第一项是正相项，推动 $a_y$ 向上；而第二项是负相项，对于所有 i 以权重 P(i|C) 推动 $a_i$ 向下。由于负相项是期望值，则可以通过蒙特卡罗采样估计。

            我们可以从另外一个分布中采样，而不是从模型中采样，这个分布叫提议分布，并通过适当的权重校正从错误分布采样引入的偏差。这个叫做重要性采样。

            重要性采样可以加速具有较大的 softmax 输出的模型，还可以加速具有大稀疏输出层的训练。

            [测试代码](./code/12-3.py)

        1. 噪声对比估计和排名损失

            为了减少训练大词汇表的神经语言模型的计算成本，有一种办法是排名损失，即将神经语言模型每一个词的输出都视为一个得分，并试图使正确词的得分比其他词的排名更高，即损失函数为：

            $$L=\sum_i max(0,1-a_y+a_i)$$

            $a_y$ 是正确词得分，$a_i$ 是其他词得分。

            但这个不提供估计的条件概率。

            最近用于神经语言模型的训练目标是噪声对比估计。

    1. 结合 n-gram 和神经语言模型

        n-gram 的优点是比神经语言模型具有更高的模型容量。而神经语言模型需要与参数数量成比例的计算量，所以增加容量的办法是两者结合。

        通过集成学习来进行组合两者。

    1. 神经机器翻译

        目前CNN和RNN都组合探索策略，基于编码器-解码器框架的总体思想。

        这种想法已经扩展到其他领域，包括为图像生成标题。

        早期，不仅传统的回退n-gram模型，还是最大熵语言模型都是基于 n-gram 模型，给定上下文最常见的词，affine-softmax 层预测下一个词。

        后来采用 MLP 替代了 n-gram，定义某些变量的边缘分布，对目标语言的短语进行评分。
        
        基于MLP的缺点是需要将序列预处理为固定长度。为了更灵活，引入了RNN，总体分为两个模型，一个模型是先读取序列并产生概括输入序列的数据结构，这个概括称为上下文C，C可以是向量或张量，另一个模型则读取上下文并且生成目标语言的句子。

        编码器 - 解码器架构：

        ```graphviz
        digraph encodedecode{
        rankdir=LR;
        node [margin=0 width=0.5 fontsize=10]

        e->c [label="编码器"]
        c->d [label="解码器"]

        e [label="源对象(句子或图像)"]
        c [label="中间的语义表示"]
        d [label="输出对象(句子)"]
        }
        ```

        1. 使用注意力机制并对齐数据片段

            用固定大小的表示很难概括非常长的句子的所有语义细节。这需要足够大的RNN，并且要足够长的训练时间。更高效的做法是，先读取整个句子或段落，以获得正在表达的上下文和焦点，然后一次翻译一个词，每次聚焦于输入句子的不同部分来手机产生一下个输出词所需的语义细节。

            基于注意力机制的系统有三个组件：

            - 读取器读取原始数据，并转换为分布式表示，其中一个特征向量与每个词的位置相关联。

            - 存储器存储读取器输出的特征向量列表。

            - 最后一个程序利用存储器的内容顺序的执行任务，每个时间步聚焦于某个存储器元素的内容。

            当用一种语言书写的句子中的词与另一种语言的翻译语句中的相应词对齐时，可以使对应的词嵌入相关联。

        [测试代码](./code/12-6.py)

        也可以训练特征向量

        [测试代码](./code/12-7.py)

    1. 历史展望

        1900 出现词嵌入

        1996 神经网络在NLP得到应用

        2001 对词进行建模，引入神经网络，产生可解释的词嵌入

        2015 现在开始对单个字符进行建模

    1. 最新的NLP模型

        1. GOOGLE BERT 模型

1. 其他应用

    1. 推荐系统

        早期的系统是基于用户ID和项目ID，如果用户1，用户2，都喜欢A、B、C，则视为用户1和用户2是同一类用户，如果用户1，又喜欢D，则表示用户2也会喜欢D。这种方法称为协同过滤。

        进行双线性预测，通过用户嵌入和项目的点击获得预计评级，然后通过对评级矩阵进行奇异值分解，降维得出预测结果。也可以使用神经网络即基于RBM的无向概率模型。

        协调过滤的限制为：当引入新项目或新用户时，缺乏评级历史，导致无法评估。这个被称为冷启动推荐问题，一般解决办法是引入单个用户和现有项目的联系。即引入用户简要信息或每个项目的特征，这种系统称为基于内容的推荐系统。


        1. 探索与利用

            当向用户推荐时，已经超过了监督学习的范围，进入了强化学习的领域。

            目前难题是，得到的数据，用户偏好有偏或不完整：只能得到用户对推荐项目的反馈，而不是其他项目；更重要的，不知道推荐其他项目有任何会产生什么结果。

            所以可以通过观察动作之后的用户反馈作为奖励来训练。即学习者（即推荐动作）和数据分布之间的反馈循环，是强化学习的范畴。

            强化学习需要权衡探索和利用。探索是指采取行动获得更多训练数据。已知给定上下文可以获得1的奖励，但不知道这个是否是最高奖励，所以进行探索，虽然可能导致动作获得0的奖励，但也有可能获得2的经验，这样就获得了一些知识。利用是指从目前学习到的最好策略采取的动作以获得最高奖励。

            探索有很多方法，从覆盖可能动作的整个空间的随机动作到基于模型的方法（基于预期回报和对该回报的不确定量来计算动作的选择）

    1. 知识表示、推理和回答

        目前使用符号和词嵌入在语言模型、机器翻译和自然语言处理方面都非常成功。不过只是单个词或概念的语义知识，目前研究前沿是短语或词和事实之间的关系开发嵌入。

        1. 知识、联系和回答

            目前有个有趣的研究是如何训练分布式表示，才能捕获两个实体之间的关系。

            目前将关系看做是高度结构化的语言，关系起到动词的作用，而关系的两个参数则是主体和客体，即为一个三元组标记：

            {主体， 动作， 客体}

            值为：

            {实体， 关系， 实体}

            还可以自定义属性，类似关系，但只有一个参数：

            {实体， 属性}

            神经网络可以训练的数据集，可以是非结构化自然语言组成的样本；也可以是结构化自然语言组成的样本，称为知识库。

            神经网络的训练模型，常见方法是将神经语言模型扩展到模型实体和关系。神经语言模型学习提供每个词分布式表示的向量。还可以进一步学习这些向量的函数来学习词和词之间的相互作用，例如哪些词在词序列之后。可以同时使用知识库和自然语言句子来训练。

            这种模型的实际短期应用是链式预测：预测知识图谱中缺失的部分。这个是基于旧事实推广新事实的一种形式。

            困难：很难评估链式预测的性能，因为数据集只有正样本，如果模型提出了一个不在数据集中的事实，很难确定是模型犯错还是发现了一个新事实。通常构造感兴趣的负样本由真实的事实产生损坏版本，例如用随机选择的不同实体替换关系中的一个实体。

            知识库和分布表示的另外一个应用是词义消歧，这个任务可以决定在某个语境中哪个词的意义是恰当的。

            知识的关系结合一个推理过程，可以建立一个问答系统。问答系统需要处理输入信息并记住重要的事实，并以之后能检索和推理的方式组织。目前最佳方案是采用显式记忆机制。






