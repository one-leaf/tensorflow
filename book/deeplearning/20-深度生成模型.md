1. 背景

    1. 传统的神经网络都是单个隐藏层，最多两个隐藏层，因为太多模型参数过大，很难训练。

    2. 传统的神经网络在隐藏层增加时，容易陷入局部最优解，也容易梯度弥散或梯度饱和。

    3. 随着神经网络的层数增加，要求有标签的数据很大，不适用于小样本能力。

1. 玻尔兹曼机

    玻尔兹曼机是用来学习二值向量上的任意概率分布。其神经元只有两种状态0、1，表示激活或未激活。

    在 $d$ 维二值随机向量 $x\in \{0,1\}^d$ 上定义玻尔兹曼机：

    利用能量函数来定义联合概率分布：

    $$P(x)=\frac {exp(-E(x))}{Z}$$ 

    其中，E(x)是能量函数，Z是保持P(x)=1的配分函数。在玻尔兹曼机中的E(x)具体为：

    $$E(x)=-x^TWx-b^Tx$$

    W是权重矩阵，b为偏置。

    将x分解为两个单元: v 和 隐藏层h，则能量函数如下：

    $$E(v,h)=-v^TRv-v^TWh-h^TSh-b^Tv-c^Th$$

    其学习采用最大似然，所以具有配分函数难处理的问题。

    玻尔兹曼机可以用在监督学习或无监督学习中，其代价是训练时间非常漫长。

1. 受限玻尔兹曼机

    玻尔兹曼机是层内都有连接的，受限玻尔兹曼机(RBM)对玻尔兹曼机(BM)进行简化，删除掉同一层之间的内连接，保留层与层之间的全连接，即：RBM由两层构成，包含一层可见层和单层隐藏层的无向概率图模型。这样简化计算，特点是在输入数据时，隐藏层的激活条件是独立的，同样在输入隐藏层的单元状态时，可见层的激活条件也是独立的，层内之间没有连接。隐藏层用于提取特征，所以也称为特征检测器，即通过学习后将得到输入数据的特征。同时还可以通过学习将输入数据表示成概率模型，这样也可以用于生成新数据。

    可见层（x），隐藏层（h），可见层到隐藏层的偏差（b）,隐藏层到可见层的偏差（c），可见层与隐藏层之间的权重矩阵（W），其联合概率分布如下：

    $$P(x,h)=\frac {exp(-E(x,h))}{Z}$$

    能量函数E定义如下：

    $$E(x,h)=-b^Tx-c^Th-x^TWh$$

    Z为配分函数的归一化函数：

    $$Z=\sum_x\sum_h exp(-E(x,h))$$

    作为无监督学习的特征提取，先随机h的权重w,加上激活函数计算x输入出正向结果，然后利用h的激活状态再乘以w的相同边反向计算出输入值，优化过程为求反向计算的输入和真实输入的散度。

    1. 条件分布

        从联合分布导出条件分布：

        $$P(h|x)=\frac{P(h,x)}{P(x)}$$

        $$=\frac {1}{P(x)}\frac {1}{Z}exp(b^Tx+c^Th+x^TWh)$$

        $$=\frac {1}{Z'}exp(c^Th+x^TWh)$$

        $$=\frac {1}{Z'}exp(\sum_{j=1}^{n_h}c^T_jh_j+\sum_{n_h}^{j=1}x^TW_{:,j}h_j)$$

        $$=\frac {1}{Z'}\prod_{j=1}^{n_h}exp(c^T_jh_j+x^TW_{:,j}h_j)$$

        上式中，我们是相对 $x$ 计算 $P(h|x)$ 的条件概率，所以可以将 $x$ 看做常数。这样，其条件分布因子相乘的本质，可以将向量 $h$ 上的联合概率写成单独元素 $h_j$ 分布的乘积。这样原始问题转为了针对单个二值 $h_j$上分布进行归一化的简单问题。

        $$P(h_j=1|x)=\frac{\hat P(h_j=1|x)}{\hat P(h_j=0|x)+\hat P(h_j=1|x)}$$

        $$=\frac{exp(c_j+x^TW_{:,j})}{exp(0)+exp(c_j+x^TW_{:,j})}$$

        $$=\sigma(c_j+v^TW_{:,j})$$

        这样就可以将隐藏层的完全条件概率分布的因子形式为：

        $$P(h|x)=\prod_{j=1}^{n_h}\sigma((2h-1)\odot(c+W^Tx))_j$$

        $$P(x|h)=\prod_{i=1}^{n_x}\sigma((2x-1)\odot(b+Wh))_i$$

    1. 训练受限玻尔兹曼机

        由于RBM容易进行估计和微分以及采用，所以相对其他无向图模型而言，可以直接以闭解形式进行计算。

1. 深度信念网络

    深度信念网络（DBN）是第一批成功应用深度架构训练的非CNN模型之一。通过采用逐层训练的方式，解决了深层次神经网络的优化问题，通过逐层训练为整个网络赋予较好的初始权重，使得网络只要经过微调就可以达到最优解。

    对DBN进行贪心逐层训练后，不需要再进行联合训练。训练好的数据可以直接用作生成模型，不过最感兴趣的是利用这个权重来改善分类模型的能力，即直接将权重定义MLP。将训练好的权重和偏置初始化MLP之后，继续训练该MLP来执行分类任务。

1. 深度玻尔兹曼机

    深度玻尔兹曼机（DBM）和深度信念网络不同，完全是一个无向图模型，也和玻尔兹曼机不同，有多个隐藏层，层内变量是相互独立的，并条件于相邻层中的变量。

    深度信念网络是顶部$x$和$h_1$之间的连接是无向的，但其他层之间的连接是有向的。深度玻尔兹曼机所有层之间的连接都是无向的。

    和RBM、DBN一样，DBM也是二值的。

    例如一个深度玻尔兹曼机包含一个可见层和3个隐藏层的联合概率如下：

    $$P(x,h_1,h_2,h_3)=\frac{1}{Z(\theta)}exo(-E(x,h_1,h_2,h_3))$$

    其函数能量简化偏置b后的定义为：

    $$E(x,h_1,h_2,h_3;\theta)=-x^TW_1h_1-h_1^TW_2h_2-h_2^TW_3h_3$$

    








