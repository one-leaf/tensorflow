结构化概率模型使用图来描述概率分布中随机变量之间的直接相互作用，从而描述一个概率分布。也称为：图模型

1. 非结构化建模的挑战

    对上千或上百万随机变量的分布建模，无论上从计算上还是从统计意义上说，都是一个极具挑战性的任务。

    通常意义上讲，如果我们希望对一个包含n个离散变量并且每个变量都能取k个值的x的分布建模，则需要$k^n$个参数，但这是不可行的。

    原因：内存、统计的高效性、推断的时间开销、采样的时间开销等

    结构化概率模型可以为随机变量之间的直接作用提供一个正式的建模框架。这种方法极大的减少了模型的参数个数，可以使用更少的数据来进行有效的估计。

1. 使用图描述模型结构

    结构化概率模型使用图来表示随机变量之间的相互作用，每一个节点代表一个随机变量，每一条边代表一个直接相互作用。

    1. 有向图模型

        有向图模型是一种结构化概率模型，也称为信念网络或贝叶斯网络。

        只所以称为有向模型，是因为所有的边都是有方向的，箭头所指的方向表示了这个随机变量的概率分布是由其他变量的概率分布所定义的。

        即：变量x的有向概率模型是通过有向无环图和一系列的局部条件概率分布来定义的。

        例如如下有向图：

        $$ t_0 \longrightarrow t_1 \longrightarrow t_2 $$

        其概率为：

        $$p(t_0,t_1,t_2)=p(t_0)p(t_1|t_0)p(t_2|t_1)$$

        如果$t_0,t_1,t_2$分别有100种可能，按表来存储参数则需要 $100*100*100-1=999999$个值空间来存储，其中概率1不需要存储，所以减去1；如果用条件概率来存储则只需要$99+99*100+99*100=19899$ 个值空间。表示使用有向图模型可以将参数的个数减少了50倍。

        通常意义上说，对于每个变量都能取k个值的n个变量模型，基于建表的模型的复杂度是$O(k^n)$,而有向图中，使用m表示图模型的单个条件概率分布中的最大变量数目，那么对于这个有向图的模型复杂度大致为$O(k^m)$，只要我们设计模型时，满足$m \ll n$，则复杂度会大大减小。

        也就是只要图中的每个变量都只有少量的父节点，那么这个分布就可以用较少的参数来表示。

    1. 无向模型

        无向模型所有的边都是没有方向的，也称为马尔可夫随机场或马尔可夫网络。

        密切度越高的状态有越大的概率。

    1. 配分函数

        将未归一化的概率函数进行归一化，使得概率之和或积分为1.

        $$p(x)=\frac{1}{Z}\tilde p(x)$$      

        Z 通常是所有的概率之和或积分为1的常数，且满足

        $$Z=\int \tilde p(x)dx$$

        当函数固定时，Z为参数，当函数带有参数时，Z为这些参数的函数，所以归一化常数Z通常称为配分函数。

        为了获得无向模型的归一化概率分布，模型的结构和函数需要设计成方便计算Z；需要记得，设定一些使得Z不存在的因子也是有可能的。  例如对$Z=\int x^2dx$建模，由于这个积分是发散的，所以不存在这样对应这个模型的概率分布。

        有向模型和无向模型的重要区别是，有向模型是通过起始点概率直接定义的，而无向模型的定义很宽松，直接将函数转为概率分布来定义。所以在处理无向模型的时候需要记得，每一个变量的定义域对一系列给定函数所对应的概率分布有着重要的影响。

    1. 基于能量的模型

        无向模型的绝大多数理论都依赖于 $\tilde p(x)>0$，满足这个特性的最简单的表达为：

        $$\tilde p(x)=exp(-E(x))$$

        其中 $E(x)$ 被称为能量函数。对于所有的能量函数的输出，exp()，都保证值是正的，同时可以无限接近于0。

        通常将许多基于能量模型的模型称为玻尔兹曼机，其分布为玻尔兹曼分布。

        通常不计算 $p_{model}(x)$,而计算$log\tilde p_{modle}(x)$，即：

        $$F(x)=-log\sum_h exp(-E(x,h))$$

    1. 分离和d-分离

        图模型的边告诉了哪些变量直接相互作用，但经常需要知道哪些变量是彼此互相独立的。

        $$ a - s - b $$

        在无向图模型中识别独立性很简单，在给定s的情况下，只要两个变量集a、b无关，就称为a和b是分离的。

        类似的概念适用于有向图模型，被称为d-分离，d的意思是依赖。

        $$ a \longrightarrow s \longleftarrow b $$

        s为没有上班，a为生病，b为休假，a和b发生都会导致s，是d-分离的；但如果给定s的情况下，如当观察到s没有上班，a和b不会同时发生，即a,b是相关的，也就不是d-分离的。

    1. 在有向模型和无向模型中转换

        在机器学习的模型中，通常把受限玻尔兹曼机称为无向模型，而稀疏编码称为有向模型。其实是不对的，没有概率模型在本质上是有向或无向的，至少适合用图模型来描述。

        有向模型提供了高效抽取样本的方法，无向模型通常对于推导近似推断过程。可以根据任务的来觉得采用有向建模还是无向建模。

        通过构造道德图，可以做到有向模型和无向模型之间的互相转化。

    1. 因子图

        因子图是从无向模型中抽样的另一种方法。用来解决标准无向模型语法中图表达的模糊性。

1. 在图模型中采样

    有向图模型的一个优点是，可以通过一个简单高效的过程从模型所表示的联合分布中产生样本，这个过程被称为原始采样。即，将图中的变量$x_i$使用拓扑排序，使得对于所有的i和j，相对应采样。原始采样优点是采样速度快，缺点是只能用于有向图模型。

1. 结构化建模的优势

    使用结构化建模的主要优点是，能够显著的降低表示概率分布、学习和推断的成本。

    结构化概率模型可以允许我们明确的将给定的现有知识与知识的学习或推断分开，可以使得容易开发和调试。

1. 学习依赖关系

    深度学习中，最常用于建模变量不同元素高度依赖关系的方法是：引入几个潜在或隐藏变量h。然后，该模型可以捕获任何对接依赖。（变量$v_i$和$v_j$的间接依赖可以通过$v_i$和$h$之间的直接依赖和$h$和$v_j$的直接依赖捕获）

1. 推断和近似推断

    解决变量之间的如何互相关联的问题是我们使用概率模型的一个主要原因。采用最大似然来训练模型，这些都是推断的例子。

    很多时候，即使采用图结构化的深度模型也很难处理，这样迫使我们采用近似推断来逼近推断。

1. 结构化概率模型的深度学习方法

    深度学习并不总是涉及特别深的图模型。

    深度学习基本上总是利用分布式的思想。深度学习与传统的图模型相比，可以观察到更多的潜变量。

    对于潜变量的设计方式，深度模型通常不希望潜变量提前赋予任何特定的含义，而图模型不同。图模型通常希望保持精确推断的可解性。

    深度模型基本上从不使用环状信念传播。

    图模型的深度学习方法还在于对未知量的较高容忍度。

    1.  实例：受限玻尔兹曼机

        受限玻尔兹曼机（RBM）是图模型如何用于深度学习的典型例子。

        其能量函数为：

        $$E(v,h)=-b^Tv-c^Th-v^TWh$$

        其中，b、c和W都是无约束、实值的可学习参数。模型分为两块，v和h，之间的关系通过W来描述。

        得出：

        $$p(h|v)=\prod_i{p(h_i|v)}$$

        和

        $$p(v|h)=\prod_i{p(v_i|h)}$$

        独立的条件分布为：

        $$p(h_i=1|v)=\sigma(v^TW_{:,i}+b_i)$$

        $$p(h_i=0|v)=1-\sigma(v^TW_{:,i}+b_i)$$

        由于能量函数只是参数的线性函数，所以求导很容易：

        $$\frac{\partial}{\partial W_{i,j}}E(v,h)=-v_ih_j$$

        应当注意的是，RBM并不是效果最稳定、最一致的浅层前馈网络。许多情况下，包含稠密层的自动编码器效果更好。整个行业也确实越来越倾向于使用变分自动编码器等工具。

        [参考代码](./code/16-1.py)







     