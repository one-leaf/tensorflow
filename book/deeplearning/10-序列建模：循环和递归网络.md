循环神经网络（RNN）是用于处理序列数据的神经网络。

最初是在一维时间序列上使用卷积，这种是时延神经网络的基础，卷积允许网络跨时间共享参数，但是浅层的。卷积输出的是一个序列，其中输出中的每一项是相邻几项输入的函数。参数共享体现在每个时间步使用了相同的卷积核。循环神经网络是不同的，输出的每一项是前一项的函数。输出的每一项对先前的输出应用相同的更新规则而产生，这种循环方式导致参数通过很深的计算图共享。

1. 展开计算图

    动态系统的经典形式：

    $$s_{(t)}=f(s_{(t-1)};\theta)$$

    s 在时刻 t 的定义需要参考 t-1 时刻的同样定义。

    例如对t=3的展开为：

    $$s_3=f(s_2;\theta)=f(f(s_1;\theta);\theta)$$

    引入外部信号$x_{(t)}$，则：

    $$s_{(t)}=f(s_{(t-1)},x_{(t)};\theta)$$

    这样可以看到当前状态包含了整个过去序列的信息

    有时为了表示神经网络的隐藏层，用h代替s为：

    $$h_{(t)}=f(h_{(t-1)},x_{(t)};\theta)$$

    如：

    ```graphviz
    digraph rnn {
        rankdir=LR;
        h [shape=circle]
        x [shape=circle]
        h -> h
        x -> h[label="f"]       
    }
    ```

    展开后为

    ```graphviz
    digraph rnn2{
        rankdir=LR;
        node [margin=0 shape=circle width=0.5 fontsize=10]
        S  [label="h(...)" style=dashed]
        h1 [label="h(t-1)" ]
        h2 [label="h(t)" ]
        h3 [label="h(t+1)" ]
        E  [label="h(...)"  style=dashed]
        x1 [label="x(t-1)" ]
        x2 [label="x(t)" ]
        x3 [label="x(t+1)" ]
        {rank=same; x1; h1}
        {rank=same; x2; h2}
        {rank=same; x3; h3}
        S->h1->h2->h3->E [label="f"]    
        x1->h1 [constraint=false]
        x2->h2 [constraint=false]
        x3->h3 [constraint=false]
    }
    ```

1. 循环神经网络

    循环神经网络的设计模式主要三种：

    - 每个时间步都有输出，并且隐藏单元之间有循环网络

        ```graphviz
        digraph rnn{
            rankdir=LR;
            node [margin=0 shape=circle width=0.5 fontsize=10]
            x->h [label="U"]
            h->h [label="W"]
            h->o [label="V"]
            o->L
            y->L [constraint=false]
            {rank=sink; y}
        }
        ```

        如图，是将输入x的序列，映射到输出o的对应序列，o是未归一化的对数概率，然后softmax(o)，计算损失L，x到rnn的隐藏层权重矩阵是U，rnn隐藏层内部循环的权重矩阵是W，rnn输出到o的权重矩阵是V。

        特点是输入和输出都是同时间步的。

        [演示代码](./code/10-1.py)

    - 每一个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接

        ```graphviz
        digraph rnn{
            rankdir=LR;
            node [margin=0 shape=circle width=0.5 fontsize=10]
            x->h [label="U"]
            h->o [label="V"]
            o->h [label="W"]
            o->L
            y->L [constraint=false]
            {rank=sink; y}
        }
        ```

        这种循环网络是将输出重新反馈到隐藏层，这样由于o是h的未来信息，比起直接在h中反馈，会丢失很多信息，好处是训练容易，可以分离时间步，方便做并行训练。

        [演示代码](./code/10-2.py)

    - 隐藏单元之间做循环连接，但读取完毕整个序列后只产生单个输出的循环网络。

        ```graphviz
        digraph rnn{
            rankdir=LR;
            node [margin=0 shape=circle width=0.5 fontsize=10]
            a [label="..."]
            h1 [label="h(t-3)"]
            h2 [label="h(...)"]
            h3 [label="h(t)"]
            x1 [label="x(t-3)"]
            x2 [label="x(...)"]
            x3 [label="x(t)"]
            o  [label="o(t)"]
            L  [label="L(t)"]
            y  [label="y(t)"]

            a->h1 [label="W"]
            h1->h2 [label="W"]
            h2->h3 [label="W"]
            h3->o [label="V"]
            o->L 
            x1 -> h1 [label="U" constraint=false]
            x2 -> h2 [label="U" constraint=false]
            x3 -> h3 [label="U" constraint=false]
            y->L [constraint=false]

            {rank=same; h1; x1}
            {rank=same; h2; x2}
            {rank=same; h3; x3}
            {rank=sink; y}
        }
        ```

        如图，在序列结束时具有单个的输出，这样可以概括序列，并产生固定大小的表示，方便后续处理。

        [演示代码](./code/10-3.py)

    反向传播：假设使用双曲线函数tanh做为rnn的激活函数，则反向传播如下：

    $$a_{(t)}=b+Wh_{(t-1)}+Ux_{(t)}$$

    $$h_{(t)}=tanh(a_{(t)})$$

    $$o_{(t)}=c+Vh_{(t)}$$

    $$\hat y_{(t)}=softmax(o_{(t)})$$

    b,c为偏置，同时连接权重矩阵 U、V、W；分别对应输入到隐藏层、隐藏层到输出和隐藏层到隐藏层的连接。

    损失函数为：

    $$L((x_1,...,x_t),(y_1,...,y_t))$$

    $$=\sum_t L_t$$

    $$=-\sum_t\log P_{model}(y_t|(x_1,...,x_t))$$

    由于是时间序列，无法并行计算，导致计算损失函数的时间成本很高，这种用于展开图且代价为O(T)的反向传播算法称为通过时间反向传播(BPTT)。

    1. 导师驱动过程和输出神经网络

        只在一个时刻步输出和下一个时间步的隐藏单元相连接，由于缺乏隐藏单元到隐藏单元的连接，不会很强大，但好处在于隐藏单元和隐藏单元进行解耦了，这样训练可以并行化。

        由输出反馈到模型而产生循环连接的模型可用导师驱动过程进行训练，导师驱动过程不适应最大似然准则，而在时刻t+1接收真实值$y_{(t)}$作为输入，即：

        $$logp(y_{(1)},y_{(2)}|x_{(1)},x_{(2)})$$

        $$=logp(y_{(2)}|y_{(1)},x_{(1)},x_{(2)})+logp(y_{(1)}|x_{(1)},x_{(2)})$$
    
        训练网络即：

        ```graphviz
        digraph rnn{
            rankdir=LR;
            node [margin=0 shape=circle width=0.5 fontsize=10]
            y1 [label="y(t-1)"]
            y2 [label="y(t)"]
            l1 [label="L(t-1)"]
            l2 [label="L(t)"]
            o1 [label="o(t-1)"]
            o2 [label="o(t)"]
            h1 [label="h(t-1)"]
            h2 [label="h(t)"]
            x1 [label="x(t-1)"]
            x2 [label="x(t)"]
            x1->h1 [label="U"]
            x2->h2 [label="U"]
            h2->o2 [label="V"]
            h1->o1 [label="V"] 
            o2->l2
            o1->l1
            y2->l2 
            y1->l1 
            y1->h2 [label="W" constraint=false]
            {rank=same; l1; l2}
            {rank=same; o1; o2}
            {rank=same; h1; h2}
            {rank=same; x1; x2}
            {rank=sink; y1;y2}
        }
        ```    

        测试时：

        ```graphviz
        digraph rnn{
            rankdir=LR;
            node [margin=0 shape=circle width=0.5 fontsize=10]
            o1 [label="o(t-1)"]
            o2 [label="o(t)"]
            h1 [label="h(t-1)"]
            h2 [label="h(t)"]
            x1 [label="x(t-1)"]
            x2 [label="x(t)"]
            x1->h1 [label="U"]
            x2->h2 [label="U"]
            h2->o2 [label="V"]
            h1->o1 [label="V"]
            o1->h2 [label="W" constraint=false]
            {rank=same; o1; h2}
            {rank=same; x2; h1}
        }
        ```    

        只要隐藏单元成为较早时间步的函数，BPTT算法是必要的，某些模型需要同时应用导师驱动过程和BPTT算法。

        主要这个模型在预测时下一步的输入完全依赖前一步输出，误差会放大。如果这个模型在开环时使用，即网络输出反馈为输入，那么训练期间看到的输入与测试时看到的输入完全不同。减轻此问题的办法是同时使用导师驱动过程和自由运行的输入进行训练。

        [演示代码](./code/10-4.py)

    1. 计算循环神经网络的梯度

        

        
